% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section{Präsenzübung 2 - Mittwoch}
\subsection{Präsenzaufgabe 2.1}

Wir betrachten den Umkreis eines gleichseitigen Dreiecks mit Seitenlänge
$\sqrt{3}$. Für den Radius des Umkreises gilt dann $r = 1$. Nun ziehen wir eine zufällige Sehne durch den Kreis. Mit welcher Wahrscheinlichkeit ist die Sehnenlänge größer als $\sqrt{3}$.
\emph{Wodurch wird eine Sehne eindeutig bestimmt?}

\begin{lösung}
	%TODO make a tikz-pic!
	\begin{itemize}
		\item $r=1$, $a = \sqrt{3}$
		\item Sehne zufällig durch den Kreis
		\item mit welcher Wahrscheinlichkeit, ist die Sehne länger als die Dreiecksseite $a$?
	\end{itemize}
	\begin{enumerate}
		\item[(1)] Durch Endpunkte: $\Meas(\abs{S} > \sqrt{3}) = \frac{1}{3}$
		\item[(2)] Radiuslevel: $\Meas(\abs{S} > \sqrt{3}) = \frac{1}{3}$
		\item[(3)] Mittelpunkt: $\Meas(\abs{S} > \sqrt{3}) = \frac{1}{3}$
	\end{enumerate}
	Meine Idee dazu, nehme die längste Sehne $S = 2$ und dann natürlich $S > \sqrt{3}$. Problem aber immer noch wie ich das mathematisch erfassen soll?
	\begin{enumerate}
		\item[(1)] Setze: $f_{\Theta(B)}(x) = \frac{1}{2\pi}\quad x \in [-\pi,\pi]$ Dichte und Länge $L(A,B) = 2\sin\brackets{\frac{1}{2}\abs{B}}$
		\begin{align*}
			\Meas(L(A,B) \le z) &= \Meas(2\sin(\Theta(B)) \le 2)\\
			&= \Meas(...)
		\end{align*}
		\item[(2)] %TODO
		\item[(3)] 
	\end{enumerate}
\end{lösung}

%%%%%%%%%%%%%%%%%%%% Aufgabe 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Präsenzaufgabe 2.2}
Wir definieren eine Äquivalenzrelation auf $\R$ durch 
$x \sim y : \Leftrightarrow x-y \in \Q$, Für all $x \in \R$ schreiben wir 
$\overline{x} := \set{y \in \R x \sim y}$ für die Äquivalenzklasse von $x$. Die Menge der Äquivalenzklassen bildet eine Partition von $\R$. Mit dem Auswahlaxiom können wir eine Menge $V \subset[0,1]$ auswählen, die einen Repräsentanten jeder Äquivalenzklasse enthält, d.h. $\abs{V \cap \overline{x}} = 1$. Diese Menge heißt Vitali-Menge. Zeigen Sie, $V$ ist nicht Lebesgue-messbar.\\
Hinweise:
\begin{itemize}
	\item  Nehmen Sie an, V wäre messbar.
	\item Zählen Sie dafür die rationalen Zahlen in $[-1.1]$ ab, $q_1,q_2,\dots$
	und betrachten Sie Mengen
	$V_k := \set{v + q k : v \in V} , k \in \N$
	\item Was gilt für $V_j \subset V_k$?
	\item Finden Sie Mengen $A,B$, sodass $A \subseteq \bigcup_{k} V_k \subseteq B$. Wählen Sie $A$ und $B$ geeignet und begründen Sie Ihre Wahl!
	\item Verwenden Sie die Eigenschaften des Lebesgue-Maßes $\mu$, um einen Widerspruch herbeizuführen.
\end{itemize}

\begin{proof}
	\begin{itemize}
		\item $V_k \cap V_j = \emptyset \quad \forall i,j \ge 1, j \neq k$
		\item $[0,1] \subseteq \bigcup_{k \ge 1} V_k \subset [-1,2]$
		\begin{align*}
			1 \le \mu\brackets{\bigcup_{k = 1} V_k} \overset{\sigma-Add}{=} \sum_{k\ge 1}^{\infty} \mu\brackets{V_k} \overset{trans.-inv}{=} \underbrace{\sum_{k\ge 1}^{\infty} \mu\brackets{{V}}}_{\infty-\sum\text{  von Konstanten}} = 3
		\end{align*}
		Das kann aber nicht sein, da das Lebesgue-Maß translationsinvariant ist, also $\mu(V_k) = \mu(V)$. Also muss $V$ nicht $\mu$-messbar sein.
	\end{itemize}
\end{proof}


%\begin{align*}
%	L_0^2:=\big\lbrace X\in L_2:\E[X]=0\big\rbrace
%\end{align*}
%der Raum der zentrierten quadrat-integrierbaren Zufallsvariablen. 
%Für diesen gilt:
%\begin{enumerate}[label=\alph*)]
%	\item $L_0^2$ ist ein Hilbertraum mit Skalarprodukt $\Cov(\cdot,\cdot)$.
%	\item Seien $X_1,...,X_n\in L_0^2$. Dann gilt:
%	\begin{align*}
%		\Var\left[\sum\limits_{j=1}^n X_j\right]
%		=\sum\limits_{j=1}^n\left(\Var[X_j]+2\cdot\sum\limits_{k>j}\Cov[X_j,X_k]\right)
%	\end{align*}
%	\item Seien $X_1,...,X_n\in L_0^2$. Dann gilt:
%	\begin{align*}
%		\sqrt{\Var\left[\sum\limits_{j=1}^n X_j\right]}\leq\sum\limits_{j=1}^n \sqrt{\Var[X_j]}
%	\end{align*}
%\end{enumerate}
%
%\begin{proof}
%	\underline{Zeige a):}\\
%	Nutze Untervektorraumkriterium:
%	\begin{itemize}
%		\item $0\in L_0^2$ ist klar.
%		\item Sei $X\in L_0^2$ und $\lambda\in\R$. Dann ist $X\in L_0^2$ wegen
%		\begin{align*}
%			\E[\lambda\cdot X]\stackeq{\text{Lin}}\lambda\cdot\E[X]\stackeq{X\in L_2^0}0
%		\end{align*}
%		\item Seien $X,Y\in L_0^2$. Dann ist $X+Y\in L_0^2$ wegen
%		\begin{align*}
%			\E[X+Y]\stackeq{\text{Lin}}\E[X]+\E[Y]\stackeq{X,Y\in L_0^2}0+0=0
%		\end{align*}
%	\end{itemize}
%	Somit ist $L_0^2$ ein Untervektorraum von $L_2$. Dass
%	\begin{align*}
%		\langle X,Y\rangle:=\Cov(X,Y):=
%		\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
%		=\E[X\cdot Y]
%		\qquad\forall X,Y\in L_0^2
%	\end{align*}
%	ein Skalarprodukt auf $L_0^2$ ist, ist klar, das es mit dem $L^2$-Skalarprodukt des Oberraumes $L^2$ übereinstimmt.
%
%	%Gleich vorweg: Die Kovarianz erfüllt sogar auf $L^2$ alle Eigenschaften bis auf positive Definitheit. Diese gilt nur auf $L_0^2$. Deswegen werden im Folgenden die Eigenschaften der Kovarianz allgemeiner gezeigt.
%	%\begin{itemize}
%	%\item Die Kovarianz ist Bilinear, denn für $\lambda\in\R$ und $X,Y,Z\in L_0^2$ gilt
%	%\begin{align*}
%	%\Cov(\lambda\cdot X,Y)
%	%&=\E\Big[\big(\lambda\cdot X-\E[ \lambda\cdot X]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\E\Big[\big(\lambda\cdot\big(X-\E[X]\big)\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\lambda\cdot\Cov(X,Y)\\
%	%\Cov(X,\lambda\cdot Y)
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(\lambda\cdot Y-\E[\lambda\cdot Y]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(\lambda\cdot\big(Y-\E[Y]\big)\big)\Big]\\
%	%&=\lambda\cdot\Cov(X,Y)\\
%	%\Cov(X+Z,Y)
%	%&=\E\Big[\big(X+Z-\E[X+Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)+\big(Z-\E[Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
%	%+\E\Big[\big(Z-\E[Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\Cov(X,Y)+\Cov(Z,Y)\\
%	%\Cov(X,Y+Z)
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y+Z-\E[Y+Z]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)+\big(X-\E[X]\big)\cdot\big(Z-\E[Z]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
%	%+\E\Big[\big(X-\E[X]\big)\cdot\big(Z-\E[Z]\big)\Big]\\
%	%&=\Cov(X,Y)+\Cov(X,Z)
%	%\end{align*}
%	%\item Die Kovarianz ist symmetrisch, was man schon an der Definition erkennt: $\Cov(X,Y)=\Cov(Y,X)$
%	%\item Die Kovarianz ist positiv definit:
%	%\begin{align*}
%	%\Cov(X,X)&=\E\Big[\underbrace{\big(X-\E[X]\big)^2}_{\geq0}\Big]\geq0\text{ und}\\
%	%\Cov(X,X)&=0\stackrel{\E[X]=0}{\gdw} X=0\text{ fast sicher}
%	%\end{align*}
%	%\end{itemize}
%	
%	Bleibt noch zu zeigen, dass $L_0^2$ bzgl. der die die Kovarianz induzierten Norm
%	\begin{align*}
%		\Vert X\Vert:=\sqrt{\Cov(X,X)}
%		=\sqrt{\E\Big[\big(X-\E[X]\big)^2\Big]}
%		=\sqrt{\E\Big[X^2\Big]}
%		\qquad\forall X,Y\in L_0^2
%	\end{align*}
%	vollständig ist. 
%	Diese Norm stimmt auf $L_0^2$ mit der bekannten $L^2$-Norm überein. 
%	Und da bekannt ist, dass $\big(L^2,\Vert\cdot\Vert_{L^2}\big)$ ein Banachraum ist, genügt es noch zu zeigen, dass $L_0^2$ abgeschlossen ist.
%	Sei also $(X_n)_{n\in\N}\subseteq L_0^2$ eine Folge mit
%	$X_n\stackrel{n\to\infty}{\longrightarrow}X\in L^2$.
%	Dann gilt schon $X\in L_0^2$, denn:
%
%	%Angenommen $\E[X]\neq0$. Dann gibt es $c>0$ so, dass
%	%\begin{align*}
%	%0<c\leq \big(\E[X]\big)^2&=\Big(\E[X]\cdot\big(\E[X-X_n+X_n]\big)\Big)\\
%	%&=\big(\E[X]\cdot\E[X-X_n]+\E[X]\cdot\underbrace{\E[X_n]}_{=0}\big)\\
%	%&=\E[X]\cdot\E[X-X_n]\\
%	%&\leq\E\big[|X|\big]\cdot\E\big[|X-X_n|\big]
%	%\stackrel{n\to\infty}{\longrightarrow}0
%	%\end{align*}
%	%Dies ist ein Widerspruch. Folglich $\E[X]=0$ und somit $X\in L_0^2(\Omega)$.\\
%
%	\begin{align*}
%		E[X]
%		&=\E[X-X_n+X_n]\\ 
%		&=\E[X-X_n]+\underbrace{\E[X_n]}_{=0}\\
%		\implies
%		\big|\E[X]\big|&\leq \E\big[|X-X_n|\big]
%		\overset{\text{Jensen}}{\leq}
%		\sqrt{\E\big[(X_n-X)^2\big]}
%		\overset{n\to\infty}{\longrightarrow} 0\\
%		\implies
%		\E[X]&=0
%	\end{align*}
%
%	\underline{Zeige b):} 
%	Für $X,Y\in L_0^2$ gilt $\Var[X]=\E\big[X^2\big]$ und $\Cov[X,Y]=\E[X\cdot Y]$. 
%	Damit folgt
%	\begin{align*}
%		\Var\left[\sum\limits_{j=1}^n X_j\right]
%		&=\E\left[\left(\sum\limits_{j=1}^n X_j\right)^2\right]\\
%		&=\E\left[\sum\limits_{j=1}^n\sum\limits_{k=1}^n X_j\cdot X_k\right]\\
%		&=\sum\limits_{j=1}^n\sum\limits_{k=1}^n \E[X_j\cdot X_k]\\
%		&=\sum\limits_{j=1}^n\left(
%		\sum\limits_{\begin{subarray}{c}k=1\\ k=j\end{subarray}}^n \E[X_j\cdot X_k]
%		+\sum\limits_{\begin{subarray}{c}k=1\\ k\neq j\end{subarray}}^n \E[X_j\cdot X_k]\right)\\
%		&=\sum\limits_{j=1}^n\left(\underbrace{\E\big[X_j^2\big]}_{=\Var[X]}+2\cdot\sum\limits_{k>j}\underbrace{\E[X_j\cdot X_k]}_{=\Cov(X_j,Y_k)}\right)
%	\end{align*}
%
%	Abstrakt als Hilbertraum aufgefasst:
%	\begin{align*}
%		\left\langle\sum\limits_{k},\sum\limits_{k} X_k\right\rangle
%		=\sum\limits_k\left\langle X_k,X_k\right\rangle+2\cdot\sum\limits_{i<j}\left\langle X_i,X_j\right\rangle
%	\end{align*}
%
%	\underline{Zeige c):}
%	\begin{align*}
%		\sqrt{\Var\left[\sum\limits_{j=1}^n X_j\right]}
%		=\sqrt{\E\left[\left(\sum\limits_{j=1}^n\right)^2\right]}
%		=\left\Vert\sum\limits_{j=1}^n X_i\right\Vert
%		\overset{\Delta\text{-Ungl}}{\leq}
%		\sum\limits_{j=1}^n\Vert X_j\Vert
%		=\sum\limits_{j=1}^n\sqrt{\Var[X_j]}
%	\end{align*}
%\end{proof}
%
%\subsection{Aufgabe 1.2}
%Seien $X,Y\in L_2(\A)$ und $\F\subseteq\A$ Unter-$\sigma$-Algebra von $\A$. 
%\textbf{Bedingte Varianz} und \textbf{bedingte Kovarianz} von $X$ bzw. $X,Y$ unter $\F$ sind definiert als
%\begin{align*}
%	\Var[X~|~\F]&:=\E\Big[\big(X-\E[X~|~\F]\big)^2~\Big|~\F\Big]\\
%	\Cov[X,Y~|~\F]&:=\E\Big[\big(X-\E[X~|~\F]\big)\cdot\big(Y-\E[Y~|~\F]\big)~\Big|~\F\Big]
%\end{align*}
%Dann gelten die Sätze der \textbf{totalen Varianz} bzw. \textbf{totalen Kovarianz}:
%\begin{align*}
%	\Var[X]&=\E\big[\Var[X~|~\F]\big]+\Var\big[\E[X~|~\F]\big]\\
%	\Cov[X,Y]&=\E\big[\Cov[X,Y~|~\F]\big]+\Cov\big[\E[X~|~F],\E[Y~|~\F]\big]
%\end{align*}
%
%\begin{proof}
%	\underline{Zur ersten Gleichung:}\\
%	Aus der Nebenrechnung
%	\begin{align}\label{eqTotaleVarianz}
%		\Var[X~|~\F]
%		\overset{\text{Def}}&=
%		\E\Big[\big(X-\E[X~|~\F]\big)^2~\Big|~\F\Big]\\\nonumber
%		\overset{\text{}}&=
%		\E\Big[X^2-2\cdot X\cdot \E[X~|~\F]+\big(\E[X~|~\F]\big)^2~\Big|~\F\Big]\\\nonumber
%		\overset{\text{Lin}}&=
%		\E\big[X^2~\big|~\F\big]-2\cdot\E\big[X\cdot\E[X~|~\F]\big]+\E\Big[\big(\E[X~|~\F]\big)^2~\Big|~\Big]\\\nonumber
%		\overset{\text{Pull-out}}&=
%		\E\big[X^2~\big|~\F\big]-2\cdot\E[X~|~\F]\cdot\E[X~|~\F]+
%		\E[X~|~\F]\cdot\underbrace{\E\big[\E[X~|~\F]~\big|~\F\big]}_{\stackeq{\text{Tower}}\E[X~|~\F]}\\\nonumber
%		&=
%		\E\big[X^2~\big|~\F\big]-\big(\E[X~|~\F]\big)^2 
%	\end{align}
%	folgt
%	\begin{align*}
%		&\E\big[\Var[X~|~\F]\big]+\Var\big[\E[X~|~\F]\big]\\
%		&=\E\big[\Var[X~|~\F]\big]+\E\Big[\big(\E[X~|~\F]\big)^2\Big]-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		\overset{\eqref{eqTotaleVarianz}}&=
%		\E\Big[\E\big[X^2~\big|~\F\big]-\big(\E[X~|~\F]\big)^2 \Big]+
%		\E\Big[\big(\E[X~|~\F]\big)^2\Big]-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		\overset{\text{Lin}}&=
%		\E\Big[\E\big[X^2~\big|~\F\big]\Big]\underbrace{-\E\Big[\big(\E[X~|~\F]\big)^2 \Big]+
%		\E\Big[\big(\E[X~|~\F]\big)^2\Big]}_{=0}-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		&=\E\Big[\E\big[X^2~\big|~\F\big]\Big]-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		\overset{\text{Tower}}&=
%		\E[X^2]-\big(\E[X]\big)^2\\
%		&=\Var[X]
%	\end{align*}
%
%	\underline{Zur zweiten Gleichung:} 
%
%	Zunächst gilt die \textit{Polarisation:} 
%	\begin{align*}
%		\Cov(X,Y)=\frac{1}{2}\cdot\big(\underbrace{\Var[X+Y]}_{=\langle X+Y,X+Y\rangle}-\underbrace{\Var[X]}_{=\langle X,X\rangle}-\underbrace{\Var[Y]}_{=\langle Y,Y\rangle}\big)
%	\end{align*}
%	Setze der Kürze halber $X_F:=\E[X~|~\F], Y_F:=\E[Y~|~\F]$. 
%	Dann gilt:
%	\begin{align*}
%		&\E[\Cov(X,y~|~\F)]\\
%		&=\E[\E[(X-X_F)\cdot(Y-Y_F)~|~\F]]\\
%		\overset{\text{Tower}}&=
%		\E[(X-X_F)\cdot(Y-Y_F)]\\
%		&=\E[X\cdot Y]-\E[\E[X~|~F]\cdot Y]-\E[X\cdot\E[Y~|~\F]]+\E[\E[X~|~\F]\cdot\E[Y~|~\F]]\\
%		\overset{\text{Sym}}&=
%		\E[X\cdot Y]-\E[X_F\cdot Y_F]\\
%		&=\Cov(X,Y)+\E[X]\cdot\E[Y]-\E[X_F\cdot Y_F]\\
%		\overset{\text{Tower}}&=
%		\Cov(X,Y)+\E[X_F]\cdot\E[Y_F]-\E[X_F\cdot Y_F]\\
%		&=\Cov(X,Y)-\Cov(\E[X~|~\F],\E[Y~|~\F]]
%	\end{align*}
%	Umstellen impliziert die Behauptung.
%\end{proof}
%
%\subsection{Aufgabe 1.3}
%Sei 
%$M=\begin{pmatrix}
%	A & B\\ C & D
%\end{pmatrix}$ eine quadratische, in Blöcke unterteilte Matrix von vollem Rang, wobei $A$ und $D$ ebenfalls quadratisch und von vollem Rang seien. 
%Die Ausdrücke
%\begin{align*}
%	(M/A):=\left(D-C\cdot A^{-1}\cdot B\right),\qquad(M/D):=\left(A-B\cdot D^{-1}\cdot C\right)
%\end{align*}
%heißen \textbf{Schurkomplement} von $A$ in $M$ bzw. 
%$D$ in $M$. Dann gilt:
%\begin{enumerate}[label=\alph*)]
%	\item $\begin{aligned}
%		M^{-1}=\begin{pmatrix}
%			(M/D)^{-1} & -A^{-1}\cdot B\cdot(M/A)^{-1}\\
%			-D^{-1}\cdot C\cdot(M/D)^{-1} & (M/A)^{-1}
%		\end{pmatrix}
%	\end{aligned}$
%	\item Sei $M$ nun symmetrisch, d.h. $A$ und $D$ sind symmetrisch und $C=B^T$. Dann gilt:
%	\begin{align*}
%		(x^T,y^T)\cdot M^{-1}\cdot\begin{pmatrix}
%			x\\y
%		\end{pmatrix}-y^T\cdot D^{-1}\cdot y=\tilde{x}^T\cdot(M/D)^{-1}\cdot\tilde{x}
%	\end{align*}
%	mit $\tilde{x}=\left(x-B\cdot D^{-1}\cdot y\right)$ für alle $x,y$ mit passender Dimension.
%	\item Es sei $(X,Y)$ multivariat normalverteilt mit Erwartungswert 0 und positiv definiter Kovarianzmatrix 
%	$\Sigma=\begin{pmatrix}
%		\Sigma_X & \Sigma_{XY}\\ 
%		\Sigma_{XY}^T & \Sigma_Y
%	\end{pmatrix}$. 
%	Dann ist $X$ bedingt auf $Y$ normalverteilt mit $\E[X~|~Y]=\Sigma_{XY}\cdot\Sigma_{Y}^{-1}$ und Kovarianzmatrix $(\Sigma/\Sigma_Y)$\\
%	Hinweis: Es gilt $\det(\Sigma)=\det(\Sigma_Y)\cdot\det(\Sigma/\Sigma_Y)$.
%\end{enumerate}
%
%\begin{proof}
%	\underline{Zeige a):}\\
%	Beachte: Matrixmultiplikation gilt Blockweise.
%	\begin{align*}
%		&M\cdot M^{-1}\\
%		&=\begin{pmatrix}
%			A & B\\ C & D
%		\end{pmatrix}\cdot\begin{pmatrix}
%			(A-B\cdot D^{-1}\cdot C)^{-1} & -A^{-1}\cdot B\cdot(D-A\cdot C^{-1}\cdot B)^{-1}\\
%			-D^{-1}\cdot C\cdot(A-B\cdot D^{-1}\cdot C)^{-1} & (D-C\cdot A^{-1}\cdot B)^{-1}
%		\end{pmatrix}\\
%		&=\begin{pmatrix}
%			(\ast) & 0\\
%			0 & (\ast\ast)
%		\end{pmatrix}\\
%		(\ast)
%		&=A\cdot(A-B\cdot D^{-1}\cdot C)^{-1}-B\cdot D^{-1}\cdot C(A-B\cdot D{-1}\cdot C)^{-1}\\
%		&=(A-B\cdot D^{-1}\cdot C)\cdot(A-B\cdot D^{-1}\cdot C)^{-1}\\
%		&=I\\
%		(\ast\ast)
%		&=(-C\cdot A^{-1}\cdot B+D)\cdot(D-C\cdot A^{-1}\cdot B)^{-1}\\
%		&=I
%	\end{align*}
%
%	\underline{Zeige b):}
%	\begin{align}\label{3b1}\tag{$\ast$} 
%		&-D^{-1}\cdot B^T\cdot(M/D)^{-1}
%		=-(M/A)^{-1}\cdot B^T\cdot A^{-1}\\\nonumber
%		&\implies
%		(M/A)\cdot D^{-1}\cdot B^T=B^T\cdot A^{-1}\cdot(M/D)\\\nonumber
%		&\implies
%		(D-B^T\cdot A^{-1}\cdot B)\cdot D^{-1}\cdot B^T=B^T\cdot A^{-1}\cdot(A-B\cdot D^{-1}\cdot B^T)\\\nonumber
%		&\implies
%		B^T-B^T\cdot A^{-1}\cdot B\cdot D^{-1}\cdot B^T
%		=B^T-B^T\cdot A^{-1}\cdot B\cdot D^{-1}\cdot B^T
%	\end{align}
%
%	\begin{align*}
%		M^{-1}
%		&=\begin{pmatrix}
%			(M/D)^{-1} & -A^{-1}\cdot B\cdot(M/A)^{-1}\\
%			-D^{-1}\cdot B^T\cdot(M/D)^{-1} & (M/A)^{-1}
%		\end{pmatrix}\\
%		\overset{\eqref{3b1}}&=
%		\begin{pmatrix}
%			(M/D)^{-1} & -A^{-1}\cdot B\cdot(M/A)^{-1}\\
%			-(M/A)^{-1}\cdot B^T\cdot A^{-1} & (M/A)^{-1}
%		\end{pmatrix}
%	\end{align*}
%	Zu zeigen:
%	\begin{align*}
%		\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}\cdot M^{-1}\cdot\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}-y^T\cdot D^{-1}\cdot y=\tilde{x}^T\cdot (M/D)^{-1}\cdot\tilde{x}
%		\mit\tilde{x}=x-B\cdot D^{-1}\cdot y
%	\end{align*}
%	Linke Seite:
%	\begin{align*}
%		x^T\cdot(M/D)^{-1}\cdot x-2 y^T\cdot\underbrace{(M/A)^{-1}\cdot B^T\cdot x}_{\text{wegen \eqref{3b1}}}\cdot x+y^T\cdot(M/A)^{-1}\cdot y-y^T\cdot D^{-1}\cdot y
%	\end{align*}
%	Rechte Seite:
%	\begin{align*}
%		x^T\cdot(M/D)^{-1}\cdot x-2 y^T\cdot D^{-1}\cdot B^T\cdot(M/D)^{-1}\cdot x+\\
%		+y^T\cdot D^{-1}\cdot B^T\cdot(M/D)^{-1}\cdot B\cdot D^{-1}\cdot y
%	\end{align*}
%	Noch zu zeigen
%	\begin{align*}
%		(M/A)^{-1}-D^{-1}
%		&=D^{-1}\cdot B^T\cdot(M/D)^{-1}\cdot B\cdot D^{-1}\\
%		&\implies %von links \cdot(M/A) und von rechts \cdot D
%		D-(M/A)=(M/A)\cdot\underbrace{ D^{-1}\cdot B^T\cdot(M/D)^{-1}}_{\stackrel{\eqref{3b1}}(M/A)^{-1}\cdot B^T\cdot A^{-1}}\cdot B
%		\\
%		&\implies
%		B^T\cdot A^{-1}\cdot B=B^T\cdot A^{-1}\cdot B
%	\end{align*}
%	
%	\underline{Zeige c):}
%	\begin{align*}
%		f_{X|Y}(x,y)
%		&=\frac{f_{XY}(x,y)}{f_Y(y)}\\
%		&=\frac{(2\cdot\pi)^{-\frac{m+n}{2}}\cdot\det(\Sigma_Y)}{\det(\Sigma)\cdot(2\cdot\pi)^{-\frac{n}{2}}}
%		\cdot\exp\left(-\frac{1}{2}\cdot\left(\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}^T\cdot\Sigma^{-1}\cdot\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}-y^T\cdot\Sigma_Y\cdot y\right)\right)\\
%		\overset{\text{(b)}}&=
%		\frac{(2\cdot\pi)^{-\frac{m}{2}}}{\det(\Sigma/\Sigma_Y}\cdot\exp\left(-\frac{1}{2}\cdot\tilde{x}^T\cdot(\Sigma/\Sigma_Y)^{-1}\cdot\tilde{x}\right)
%		\mit\tilde{x}=x-\Sigma_{XY}\cdot\Sigma_{Y}^{-1}\cdot y
%	\end{align*}
%	ist Dichte von 
%	\begin{align*}
%		\mathcal{N}\Big(\Sigma_{XY}\cdot\Sigma_Y^{-1}\cdot y,(\Sigma/\Sigma_Y)\Big)
%	\end{align*}
%\end{proof}
%
%\subsection{Aufgabe 1.4}
%In einer bestimmten Population sei das Alter $X$ bei erstmaliger Berufsunfähigkeit exponentialverteilt mit Parameter $\lambda>0$. 
%Für eine Versicherungsgesellschaft die gegen Berufsunfähigkeit versichert ist das mittlere Alter bei Eintritt der Berufsunfähigkeit von Bedeutung, 
%unter der Bedingung dass die Berufsunfähigkeit zwischen den Altersgrenzen $0\leq a\leq b$ eintritt.\\
%Bestimme diesen bedingten Erwartungswert $\E[X~|~a\leq X\leq b]$.
%
%\begin{proof}
%	Die Zufallsgröße $X\colon\Omega\to\R$ hat Dichte $f_X(x)=\lambda\cdot\exp(-\lambda\cdot x)$
%	\begin{align*}
%		\E[X~|~a\leq X\leq b]
%		&=\frac{\E\left[X\cdot\indi_{\lbrace a\leq X\leq b\rbrace}\right]}{\P[a\leq X\leq b]}\\
%		&=\frac{\int\limits_a^b \lambda\cdot x\cdot\exp(-\lambda\cdot x)\d x}{\int\limits_a^b\lambda\cdot\exp(-\lambda\cdot x)\d x}\\
%		&=\frac{\int\limits_a^b x\cdot\exp(-\lambda\cdot x)\d x}{\int\limits_a^b\exp(-\lambda\cdot x)\d x}\\
%		&=\frac{\left[\left(-\frac{x}{\lambda}-\frac{1}{\lambda^2}\right)\cdot\exp(-\lambda\cdot x)\right]_{x=a}^b}
%		{\left[-\frac{\exp(-\lambda\cdot x)}{\lambda}\right]_{x=a}^b}\\
%		%&=\frac{
%		%\frac{b\cdot\exp(-\lambda\cdot b)}{\lambda}
%		%+\frac{\exp(-\lambda\cdot b)}{\lambda^2}
%		%-\frac{a\cdot\exp(-\lambda\cdot a)}{\lambda}
%		%-\frac{\exp(-\lambda)\cdot a)}{\lambda^2}
%		%}{
%		%\frac{\exp(-\lambda\cdot b)}{\lambda}
%		%-\frac{\exp(-\lambda\cdot a)}{\lambda}}\\
%		%&=\frac{\left(b+\frac{1}{\lambda}\right)\cdot\exp(-\lambda\cdot b)-\left(a+\frac{1}{\lambda}\right)\cdot\exp(-\lambda\cdot a)}{\exp(-\lambda\cdot b)-\exp(-\lambda\cdot a)}
%		&=\frac{\frac{(a\cdot\lambda+1)\cdot\exp(-\lambda\cdot a)}{\lambda^2}-\frac{(b\cdot\lambda+1)\cdot\exp(-\lambda\cdot b)}{\lambda^2}}{\frac{\exp(-\lambda\cdot a)}{\lambda}-\frac{\exp(-\lambda\cdot b)}{\lambda}}\\
%		&=\frac{(b\cdot\lambda+1)\cdot\exp(a\cdot\lambda)-(a\cdot\lambda+1)\cdot\exp(b\cdot\lambda)}{\big(\exp(a\cdot\lambda)-\exp(b\cdot\lambda)\big)\cdot\lambda}\\
%		\overset{\text{Prof}}&=
%		\frac{1}{\lambda}+\frac{a\cdot\exp(-\lambda\cdot a)-b\cdot\exp(-\lambda\cdot b)}{\exp(-\lambda\cdot a)-\exp(-\lambda\cdot b)}
%	\end{align*}
%\end{proof}
%
%\subsection{Aufgabe 1.5}
%Welche der folgenden in der Vorlesung definierten mathematischen Objekte sind: 
%reelle Zahlen, Zufallsvariablen, messbare Funktionen von $\R\to\R$?
%\begin{align*}
%	\E[X~|~\F],\qquad\P[A~|~B],\qquad \E[X~|~Y=y],\qquad\P[A~|~\F],\qquad\E[X~|~Y]
%\end{align*}
%Wie üblich bezeichnet $\F$ eine $\sigma$-Algebra, $X,Y$ Zufallsvariablen, $y$ eine reelle Zahl und $A,B$ Ereignisse.
%
%\begin{lösung}
%	\begin{align*}
%		&\E[X~|~\F]\in L_2(\Omega,\F,\P)\text{, ist also eine Zufallsvariable}\\
%		&\P[A~|~B]\in[0,1]\subseteq\R\\
%		&\E[X~|~Y=y]:=\int\limits_{\R^m}x\cdot f_{X|Y}(x,y)\d x\in\R\\
%		&\P[A~|~\F]:=\E[\indi_A~|~\F]\in L_2(\Omega,\F,P)\text{ bzw. }\in(0,1)\text{ für festes }\omega\in\Omega\\
%		&\E[X~|~Y]:=\E[X~|~\sigma(Y)]\in L_2(\Omega,\F,\P)\text{, ist also ein Spezialfall der ersten Zeile}
%	\end{align*}
%\end{lösung}
