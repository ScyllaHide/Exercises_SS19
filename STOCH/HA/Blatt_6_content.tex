% !TeX spellcheck = en_US
% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section{6th Homework STOCH}
%%%%%%%%%%%%%%%%%%%% Aufgabe 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
%https://stats.stackexchange.com/questions/85363/simple-examples-of-uncorrelated-but-not-independent-x-and-y
\begin{solution}
	Let $X = \set{-1,1}$ with $\P(X=-1) = \P(X=1) = \sfrac{1}{2}$ and $Y = \set{-1,0,1}$ with $\P(X=1, Y=1) = \P(X=1, Y=-1) = \P(X=1,Y=0) = \sfrac{1}{2}$. Let $X$ and $Y$ be the dependent from each other with $y = 0$ if $X=-1$ and $Y = -1$ or $X=1$. Then we have for the expected values:
	\begin{align*}
		\E[X] &= \sum_{x_1 \in X}x_k \mal \P(X=x_k) = 0\\
		\E[Y] &= \sum_{y_k \in Y}y_k \mal \P(Y=y_k) = 0
		\intertext{and}
		\E[XY] &= \sum_{\substack{x_k \in X \\ y_k \in Y}} x_k \mal y_k \mal \P(X=x_k, Y=y_k)\\
		%&= 1\mal 1 \P(X=1,Y=1) + (-1)\mal 1 \mal \\&\mal\P(Y=1,Y=1) +(-1)\mal 1 +\\&+\P(X=1,Y=-1)+(1)\mal 0\P(X=-1, Y=0)\\
		&= 1\mal 1 \frac{1}{2} -1\mal 1 \mal \frac{1}{2} + 0\\
		&= 0
		\intertext{follows for}
		\Cov(X,Y) = \E[X]\E[Y] - \E[XY] = 0
	\end{align*}
	But $X,Y$ are dependent and then follows
	\begin{align*}
		\Corr(X,Y) = \frac{\Cov(X,Y)}{\Var(X)\Var(Y)} = \frac{0}{\Var(X)\Var(Y)} = 0
	\end{align*}
\end{solution}

%%%%%%%%%%%%%%%%%%%% Aufgabe 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	The discrete expected value is defined by
	\begin{align*}
		\E[X] = \sum_{x_k \in \O} x_k \P_X(x_k)
	\end{align*}
	Want to show
	\begin{align*}
		\E[X] = \sum_{k \ge 1} \P(X\ge k).
	\end{align*}
	\begin{align*}
		\sum_{k \ge 1} \P(X\ge k) &= \sum_{k \ge 1} (\P(X > k) + \P(X=k)\\
		&= \sum_{k \ge 1} \P(X>k) + \sum_{k \ge 1} \P(X=k)\\
		&= (\P(X>1) + \P(X>2) + \dots \P(X>k)) + \sum_{k\ge 1} \P(X=k)\\
		&= \text{ \ref{eq:6_2a} } + \text{ \ref{eq:6_2b} } + \dots + \text{ \ref{eq:6_2c} } +\sum_{k\ge 1} \P(X=k)\\
		&= (\P(X=2) + 2\P(X=3) + 3\P(X=4) + \dots) + + \sum_{k\ge 1} \P(X=k)\\
		&= (\P(X=2) + 2\P(X=3) + 3\P(X=4) + \dots ) + + \sum_{k\ge 1} \P(X=k)\\
		&= \sum_{k\ge 2} (k-1)\P(X=k) +\sum_{k\ge 1} \P(X=k)\\
		&= \sum_{k\ge 1} k\mal\P(X=k)\\
		&= \E[X]
	\end{align*}
	With the help of the following calculations:
	\begin{align*}
		\P(X>1) &=(\P(X=2) + \P(X=3) + \P(X=4) + \dots)\tag{$\star$}\label{eq:6_2a}\\
		\P(X>2) &=(\P(X=3) + \P(X=4) + \P(X=5) + \dots)\tag{$\star\star$}\label{eq:6_2b}\\
		\intertext{general}
		\P(X>k) &= \sum_{i=k} \P(X=i+1) \label{eq:6_2c}\tag{$\alpha$}
	\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%% Aufgabe 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}\
	\begin{enumerate}
		\item Is false, because there exists a counterexample, we can set $\Omega = \set{1,\dots,6}$ (roll a fair dice) with $X,Y \sim \Uni(\Omega)$
		\begin{align*}
			X = 
			\begin{cases}
				1 &\quad \text{dice number even}\\
				0 &\quad \text{dice numebr odd}
			\end{cases}
			\quad Y =
			\begin{cases}
				1 &\quad \text{dice number odd}\\
				0 &\quad \text{dice numebr even}
			\end{cases} 
		\end{align*}
		Then we can calculate the $\E[X] = \E[Y] =3\mal \sfrac{1}{6}= \sfrac{1}{2}$, but $\P(X=Y) = 0 \neq 1$.
		\item We can use MINT 10.2 ($\star$) and get
		\begin{align*}
			\E[\abs{X-Y}] = 0
			\xRightarrow{(\star)} \abs{X-Y} = 0 \text{ almost surely}
			\xRightarrow{(\star)} \P(X-Y)\neq 0 
		\end{align*}
		and with that follows the claim we desired to proof.
	\end{enumerate}
\end{proof}
%%%%%%%%%%%%%%%%%%%% Aufgabe 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	\begin{enumerate}
		\item\
		\begin{enumerate}
			\item \ul{pgf:} From Definition 5.14 from lecture we have
			\begin{align*}
				\psi_X(s) = \sum_{x \ge 0} \rho(x)s^x
				\intertext{and from definition of the Poisson distribution we get}
				\rho_X(k) = \frac{e^{-\lambda}\lambda^k}{k!} \quad \forall k \in \N, k\ge 0.
				\intertext{Follows}
				\psi_X(s) &= \sum_{k \ge 0} \frac{e^{-\lambda}\lambda^k}{k!}s^k\\
				&=e^{-\lambda} \sum_{k \ge 0} \frac{(\lambda s)^k}{k!}\\
				&=e^{-\lambda}e^{\lambda s} \quad \text{ Taylor Exp for $e$-function}\\
				&= e^{-\lambda + \lambda s}
			\end{align*}
			\item \ul{exptected value:}
			\begin{align*}
				\E[X] = \psi'_X(s=1) = \lambda\psi_X(s=1) = \lambda
			\end{align*}
			\item \ul{variance:}
			\begin{align*}
				\Var X &= \psi''_X(1) + \psi'_X(1) - (\psi'_X(1))^2\\
				&= \lambda^2 + \lambda - \lambda^2\\
				&= \lambda
			\end{align*}
		\end{enumerate}
	\item No idea ...
	\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	Let $X,Y \in \L^2(\O, \F, \meas)$ real random variables.
	\begin{enumerate}
		\item \ul{To show:} $\Var(X+Y) - \Var(X-Y) = 4\Cov(X,Y) = 4\brackets{\E[XY] - \E[X]\E[Y]}$
		\begin{align*}
			\Var(X+Y)- \Var(X-Y) &= \E[(X+Y)^2]-\E[X+Y]^2 - \E[(X+Y)^2] - \E[X-Y]^2\\
			&= \E[X^2+2XY+Y^2] - (\E[X] - \E[Y])^2 \\&- \E[X^2-2XY+Y^2] - (\E[X] -\E[Y])^2\\
			&= \cancel{\E[X^2]} +2\E[XY] + \cancel{\E[Y^2]} - \E[X]^2 - 2\E[X]\E[Y] - \E[Y]^2\\
			&- \cancel{\E[X^2]} + 2\E[XY] - \cancel{\E[Y^2]} + \cancel{\E[X]^2} - 2\E[X]\E[Y] + \cancel{\E[Y]^2}\\
			&= 4\E[XY] - 4\E[X]\E[Y]\\
			&= 4\E[XY] - \E[X]\E[Y].
		\end{align*}
		\item \ul{To show:} $\Var(X+Y) = \Var(X) + \Var(Y) + 2\Cov(X,Y)$
		\begin{align*}
			2\Cov(X,Y) &= \E[XY] - \E[X]\E[Y] \label{eq:6_5a}\tag{$\bigwedge$}\\
			\Var(X) &= \E[X^2] - \E[X]^2 \label{eq:6_5b}\tag{$\bigvee$}\\
			\Var(X+Y) &= \E[(X+Y)^2] - \E[X+Y]^2\\
			&= \E[X^2] + 2\E[XY] + \E[Y^2] - \E[Y]^2 - 2\E[X]\E[Y] - \E[Y]^2\\
			&= \Var(X) + \Var(Y) - 2\Cov(X,Y) \text{ used \ref{eq:6_5a} and \ref{eq:6_5b}}\\
		\end{align*}
		\item Prove the equality first:\\
			Let $a = \E[X]$ and we going to use the linearity from prop. 5.6
			\begin{align*}
				\E[(X-a)^2] &= \E[(X-\E[X])^2]\\
				&= \E[X^2 -2\E[X]x + \E[X]^2]\\
				&= \E[X^2]-2\E[2\E[X]\mal X] + \E[\E[X]^2] \text{ used } \E[\E[x]] = \E[X]\\
				&= \E[X^2] - 2\E[X]\mal \E[X] + \E[\E[X]^2]\\
				&= \E[X^2] - 2\E[X]^2 +\E[X]^2\\
				&= \E[X^2] - \E[X]^2\\
				&= \Var(X)
			\end{align*}
			Prove the inequality and use again linearity from prop 5.6 ($\star$):
			\begin{align*}
				(\E[X] - a)^2 &\ge 0 \\
				\E[X]^2 - 2a\E[X] + a^2 &\ge 0\\
				-2a \E[X] + a^2 &\ge - \E[X]^2 \\
				\E[X^2] - 2a\E[X] + \E[a^2] &\ge \E[X^2] - \E[X]^2 \quad \E[a^2] = a^2 \;(\star)\\
				\E[X^2] - \E[2aX] + \E[a^2] &\ge \E[X^2] - \E[X]^2\\
				\E[X^2-2aX+a^2] &\ge \E[X^2] - \E[X]^2\\
				\E[(x-a)^2] &\ge \Var(X)\\
			\end{align*}
			we have used $\E[(X-a)^2] = \E[X^2] - 2a[X] + \E[a^2]$, follows from linearity.
	\end{enumerate}
\end{proof}

\subsection{*}
\begin{proof} Let $X, Y \in L^2$
	\begin{enumerate}
		\item 
		\begin{enumerate}
			\item \ul{symmetry:} Need to show: $\Cov(X,Y) = \Cov(Y,X)$
			\begin{align*}
				\Cov(X,Y) &= \E[XY] - \E[X]\E[Y]\\
				&= \int_{\O} X(\omega)Y(\omega)\P(\d\omega) - \int_{\O} X(\omega)\P(\d\omega)\mal \int_{\O} Y(\omega)\P(\d \omega)\\
				\overset{(\star)}&{=} \int_{\O} Y(\omega)X(\omega)\P(\d\omega) - \int_{\O} Y(\omega)\P(\d \omega)\mal \int_{\O} X(\omega)\P(\d \omega)\\
				&= \E[YX] - \E[Y]\E[X] = \Cov(Y,X) 
			\end{align*}
			$(\star)$ follows f integral properties.
			\item \ul{positive-semidefinite:} Need to show $\Cov(X,X) \ge 0 \forall X$ r.v.
			\begin{align*}
				\Cov(X,X) &= \E[XX] - \E[X]\E[X] = \E[X^2] - \E[X]^2\\
				&= \Var(X)
			\end{align*}
			Dont know how to argue that it should be always $\ge 0$.
			\item \ul{BLF}: Let $x_1, x_2, y_1,y_2$ r.v. and $\lambda, \mu \in \R$. Going to use linearity from prop 5.6 again.
			\begin{align*}
				\Cov(\lambda(x_1 + x_2),\mu(y_1+y_2)) &= \E[\lambda(x_1 + x_2)\mu(y_1+y_2)]\\ &-\lambda\mu\E[(x_1 + x_2)]\E[y_1+y_2]\\
				&= \lambda\mu\{\E[x_1 y_1] - \E[x_1]\E[y_1] + \E[x_1 y_2] - \\&-\E[x_2]\E[y_1] + \E[x_2 y_1] - \E[x_2]\E[y_1] + \E[x_2 y_2] -\\&- \E[x_2]\E[y_2]\}\\
				&= \lambda\mu \{ \Cov(x_1,y_1) + \Cov(x_1,y_2) +\\&+ \Cov(x_2, y_1) + \Cov(x_2,y_2) \}
			\end{align*}
		\end{enumerate}
	\item 
		\begin{enumerate}
			\item \ul{$\scaProd{X}{X}$:} is scalar product. Let $x_1, x_2, y_1, y_2$ be r.v. and $\lambda, \mu \in \R$
			\begin{align*}
				\scaProd{\lambda(x_1+x_2)}{\mu(y_1+y_2)} &= \E[\lambda\mu(x_1+x_2)(y_1+y_2)]\\
				&= \lambda\mu \{ \E[x_1]\E[y_1] + \\&+\E[x_2]\E[y_1] + \E[x_1]\E[y_2] + \E[x_2]\E[y_2]\}\\
				&= \lambda \mu \{ \scaProd{x_1}{y_1} + \scaProd{x_2}{y_1} + \scaProd{x_1}{y_2} + \scaProd{x_2}{y_2}\}
			\end{align*}
			\item \ul{$\Ln{2}$ subspace of $L^2$:} We showed this in MINT already.
			\item \ul{$(\Ln{2}, \scaProd{\cdot}{\cdot})$ is complete:} The defined scalar product on $\Ln{2}$ induced a norm in the following way:
			\begin{align*}
				\norm{X} &:= \sqrt{\E[X^2]}\\
				&=\brackets{ \int_{\O} \abs{X(\omega)}^2 \P(\d \omega)}^{\frac{1}{2}} \quad\abs{\mal}\text{ here because } X^2 >0 \text{ always}\\
				&= \norm{X}_{\Ln{2}}
			\end{align*}
			Now we can use Riesz-Fischer Theorem ($\nearrow$ MINT 14.10) and this will give that $\Ln{2}$ is complete with $\norm{\mal}_{\Ln{2}}$
		\end{enumerate}
	\end{enumerate}
\end{proof}