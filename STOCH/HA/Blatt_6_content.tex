% !TeX spellcheck = en_US
% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section{6th Homework STOCH}
%%%%%%%%%%%%%%%%%%%% Aufgabe 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
%https://stats.stackexchange.com/questions/85363/simple-examples-of-uncorrelated-but-not-independent-x-and-y
\begin{proof}
	Let $X \sim \Uni((-1,1))$ and $Y = X^2$ are dependent. And they are uncorrelated.
	Calculate the $\Corr(X,Y) = 0$!
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
%https://en.wikipedia.org/wiki/Expected_value Countably infinite case
\begin{proof}
	
\end{proof}
%%%%%%%%%%%%%%%%%%%% Aufgabe 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	
\end{proof}
%%%%%%%%%%%%%%%%%%%% Aufgabe 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	\begin{enumerate}
		\item\
		\begin{enumerate}
			\item \ul{pgf:} From Definition 5.14 from lecture we have
			\begin{align*}
				\psi_X(s) = \sum_{x \ge 0} \rho(x)s^x
				\intertext{and from definition of the Poisson distribution we get}
				\rho_X(k) = \frac{e^{-\lambda}\lambda^k}{k!} \quad \forall k \in \N, k\ge 0.
				\intertext{Follows}
				\psi_X(s) &= \sum_{k \ge 0} \frac{e^{-\lambda}\lambda^k}{k!}s^k\\
				&=e^{-\lambda} \sum_{k \ge 0} \frac{(\lambda s)^k}{k!}\\
				&=e^{-\lambda}e^{\lambda s} \quad \text{ Taylor Exp for $e$-function}\\
				&= e^{-\lambda + \lambda s}
			\end{align*}
			\item \ul{exptected value:}
			\begin{align*}
				\E[X] = \psi'_X(s=1) = \lambda\psi_X(s=1) = \lambda
			\end{align*}
			\item \ul{variance:}
			\begin{align*}
				\Var X &= \psi''_X(1) + \psi'_X(1) - (\psi'_X(1))^2\\
				&= \lambda^2 + \lambda - \lambda^2\\
				&= \lambda
			\end{align*}
		\end{enumerate}
	\item Let $\O = \set{2, \dots, 12}$ and $\Uni(\O)$. We can use prop. 3.26 to get a contradiction.
	Assume there exists a prob. measure $Q$, such that holds
	\begin{align*}
		Q \star Q &= \Uni(\Omega)\\
		\sum_{l \in \O} \rho_1(l)\rho_2(l-k) &= \Uni(\O)\\
		\frac{1}{11} \sum_{l \in \O} \frac{1}{l-k} &= \Uni(\O)\\
	\end{align*}
	\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	Let $X,Y \in \L^2(\O, \F, \meas)$ real random variables.
	\begin{enumerate}
		\item \ul{To show:} $\Var(X+Y) - \Var(X-Y) = 4\Cov(X,Y)$
		
	\end{enumerate}
\end{proof}

\subsection{*}
\begin{proof} Let $X,\tilde{X}, Y, \tilde{Y} \in L^2$
	\begin{enumerate}
		\item 
		\begin{enumerate}
			\item \ul{symmetric:} $\Cov(X,Y) = \Cov(Y,X)$, because when we look at the definition of the $\Cov(X,Y) = \E[XY] - \E[X]\E[Y]$ we can simply rely on the multiplication s.t.
			$\Cov(X,Y) = \E[XY] - \E[Y]\E[X]$, the second there is just scalar multiplication is commutative and the first one just ``vector multiplication is commutative'' %TODO make this more precise, right now i dont have the right words for that
			\item \ul{bilinear form:} We need to show the four axioms for BLFs, see for example ($\nearrow$ LAAG2 Fehm), we can use linearity of the expected value in $\nearrow$ prop. 5.6.
			\item \ul{positiv-semi-definite:} We need to show that $\Cov(X,X) \ge 0$ always.
			\begin{align*}
				\Cov(X,X) &= \E[X\mal X] - \E[X]\E[X]\\
				&= \E[X]\E[X] - \E[X]\E[X] = 0
			\end{align*}
			But why does ``$>$'' hold? \label{6:6_a}
		\end{enumerate}
	\item In \ref{6:6_a} we showed that $L^2$ has ``nice'' scalar product with $\Cov(X,Y)$, but this makes $L^2$ only into a pre-hilbert-space. In order to be complete, we would require that in $L^2$ all $L^2$-Cauchy-sequences converge. ($\nearrow$ MINT Riesz-Fischer Theorem 14.10). We would need to identify the $L^2$-CS and then check if everything fits in.
	\end{enumerate}
\end{proof}