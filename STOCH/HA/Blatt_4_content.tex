% !TeX spellcheck = en_US
% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section{4th Homework STOCH}
\subsection{}
\begin{proof}[\textbf{Proof.}]\
	\begin{enumerate}
		\item Let $X \sim \Bin(n,p)$ and $Y\sim \Bin(m,p)$. Need to show that $X+Y \sim \Bin(n+m, p)$, in other words, $\Bin$ is convolution stable.
		\begin{align*}
			\meas(X + Y = N) &= \sum_{l=0}^N \meas(X=l, Y = N-l) \with m+n \le N: m+n \in \set{0,1,\dots, N}\\
			&= \sum_{l=0}^N \meas(X=l)\meas(N-l) \quad \text{ used that } X,Y \text{ are independent}\\
			&= \sum_{l=0}^N \binom{n}{l}p^l (1-p){n-l}\binom{m}{N-l}p^{N-l}(1-p)^{m - (N-l)}\\
			&= \sum_{l=0}^N \binom{n}{l}\binom{m}{N-l} p^{l+n-L}(1-p)^{n+m-l-N+l}\\
			&= \sum_{l=0}^N\binom{n}{l}\binom{m}{N-l}p^N (1-p)^{n+m - N}\\
			&= p^N (1-p)^{n+m-N}\sum_{l=0}^N \binom{n}{l}\binom{m}{N-l} \quad \text{ use Vandemond Identity}\\
			&= p^N (1-p)^{n+m-N}\mal \binom{n+m}{l+N-1}\\
			&= p^N (1-p)^{n+w-N}\mal \binom{n+m}{l}\\
			& \sim \Bin(n+m,p)
		\end{align*}
		Now a proof for the \emph{Vandermonde's Identity}.\\
		For $k,m,n \in \N$ holds
		\begin{align*}
			\binom{n+m}{k} = \sum_{j=0}^k \binom{n}{j}\mal \binom{m}{k-j}.
		\end{align*}
		\begin{proof}[\textbf{Proof.}]
			Let $N$ and $M$ disjoint sets with $\abs{N} = n \and \abs{M} = m$. For $j = 0,1,\dots,k$ let
			\begin{align*}
				P_j := \set{P \subseteq N \bigcupdot : \abs{P} = k \and \abs{P \cap N} = j\;(\and \abs{P\cap M} = k-j)},
			\end{align*}
			There are $\binom{n}{j}$ many $j$-elements subsets of $N$ and $\binom{m}{k-j}$ many $(k-j)$-elements subsets of $M$. For die cardinality of $P_j$ holds, that $\abs{P_j} = \binom{n}{j} \mal \binom{m}{k-j}$. And the sets $P_0, P_1, \dots, P_k$ pairwise disjoint. With the help of the Sum rule follows
			\begin{align*}
				\binom{n+m}{k} = \sum_{j=0}^k \abs{P_j} = \sum_{j=0}^k \binom{n}{j} \mal \binom{m}{k-j}.
			\end{align*}
		\end{proof}
		\item Let $X,Y \sim \Pois(\lambda)$. Calculate $\meas(X=k \mid Y = n -k)$:
		\begin{align*}
			\meas(X = k \mid Y + X=n) &= \meas(X=k \mid Y=n-k)\\
			&= \frac{\meas(X=k, Y = n-k)}{\meas(Y = n - k)}\\
			&= \frac{\meas(X=k)\meas(Y = n-k)}{\meas(Y=n-k)} \quad \text{ used that $X,Y$ are independet}\\
			&= \meas(X=k)\\
			&= \frac{\lambda^k}{k!} e^{-\lambda}
		\end{align*}
	\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{solution}
	Let $X \sim \Geom(p), \meas(X=k) = p(1-p)^k$.\\
	To show: $\meas(X \ge i+j \mid X \ge i) = \meas(X \ge j)$
	\begin{align*}
		\meas(X \ge i+j, X\ge i) &= \frac{\meas(x \ge i+j, X \ge i)}{\meas(X \ge i)}\\
		&= \frac{\sum_{k=i+j}^{\infty} p(1-p)^k}{\sum_{k=i}^{\infty} p(1-p)^{\infty} (1-p)^k}\\
		&= \frac{(1-p)^{i+j} \sum_{k=i+j}(1-p)^k}{(1-p)^i \sum_{k=0}^{\infty (1-p)^k}} \qquad \text{ geom. series}\\
		&= (1-p)^i\\
		&= \frac{p(1-p)^j}{p}\\
		&= p(1-p)^j \mal \sum_{k=0}^{\infty} (1-p)^k\\
		&= \sum_{k=0}^{\infty} p(1-p)^k = \meas(X \ge j).
	\end{align*}
\end{solution}
%%%%%%%%%%%%%%%%%%%% Aufgabe 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{solution}
	\begin{enumerate}
		\item Consider\
		\begin{align*}
			\meas(Y \text{ even}) &= \meas(Y=2) = \frac{1}{27} + \frac{1}{9} = \frac{4}{27}\\
			\meas(X\mal Y \text{ odd}) &= \meas(\set{(-1,-1), (-1,1),(-1,5),(1,-1),(1,1),(1,5),(5,-1),(5,1),(5,5)})\\
			&= \frac{23}{27}
		\end{align*}
		\item Set $\meas(X+Y = k) = \sum_{i=0}^2 \meas(X=l_i , Y= k -l_i) = \sum_{i=0}^2 \meas(\set{X=l_i} \cap \set{Y = k-l_i})$ with $k \in \set{-2,0,1,2,3,4,6,7,10}$
		\begin{align*}
			\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
				\hline
				k               & -2             & 0              & 1              & 2             & 3             & 4                & 6              & 7            & 10\\ \hline
				$\meas(X+Y=k)$  & $\frac{1}{27}$ & $\frac{2}{9}$  & $\frac{1}{27}$ & $\frac{2}{9}$ & $\frac{1}{9}$ & $\frac{7}{27}$   & $\frac{1}{9}$  & 0            & 0
			\end{tabular}
		\end{align*}	
		\item We have 
		\begin{align*}
			\meas(X=1,Y=5) &= \meas(X=1) \mal \meas(Y=5)\\
			\frac{1}{9}&\neq \underbrace{\frac{4}{9}}_{\sum_{i=0}^3 \meas(X=1, Y=l_i) \text{(Zeilensumme)}} \mal \underbrace{\frac{1}{9}}_{\text{Spaltensumme}}
		\end{align*}
		And this gives us a contradiction, so $X$ and $Y$ are dependent.
	\end{enumerate}
\end{solution}
%%%%%%%%%%%%%%%%%%%% Aufgabe 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{solution}
	Set
	\begin{align*}
	\meas(X=-1) = \frac{1}{2} \qquad& \meas(Y=0) = \frac{1}{2}\\
	\meas(X=1) = \frac{1}{2} \qquad& \meas(Y=1) = \frac{1}{2}
	\end{align*}
	\begin{align*}
	\meas(X,Y) &= \frac{1}{9} \for (X,Y) \in \set{(-1,0),(1,1)}\\
	\meas(X=-1, Y=1) &= 0 \neq \meas(X=-1)\mal \meas(Y=-1) = \frac{1}{4}\\ 
	&\implies X,Y \text{ dependent}\\
	&(X^2, Y^2) \in \set{(1,0), (1,1)}
	\intertext{Then we have}
	\meas(X^2 =1) &= 1\\
	\meas(Y^2 = 0) &= \frac{1}{2}\\
	\meas(Y^2 = 1) &= \frac{1}{2}\\
	\intertext{Then we have}
	\meas(X^2 =1, Y^2 = 0) = \frac{1}{2} &= \frac{1}{2} = 1 \mal \frac{1}{2}\\
	\meas(X^2 = 1, Y^2 = 1) &= \frac{1}{2} = 1 \mal \frac{1}{2}.\\
	\intertext{So we have $X^2,Y^2$ are independent as desired.} 
	\end{align*}
\end{solution}

%%%%%%%%%%%%%%%%%%%% Aufgabe 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{*}
Buffon needle problem for a short needle.
\begin{solution} %TODO take a look if a is really t compared to wikipedia
	Let $d$ be the distance from the center of the needle to the closest parallel line $y_k = 2ak,k \in \Z$ and $\alpha$ the acute angle between the needle and the one of the parallel lines $y_k$. These are the two random variables with the uniform probability density functions.\\
	For $x\in [0,\frac{a}{2}]$:
	\[
		F(x) = \begin{cases}
		\frac{2}{a} \quad& 0 \le x \le \frac{a}{2}\\
		0 \quad & \text{ else}.
		\end{cases}
	\]
	For $\alpha \in [0,\frac{\pi}{2}]$:
	\[
		F(\alpha) = \begin{cases}
		\frac{4}{a\pi} \quad& 0 \le \alpha \le \frac{\pi}{2}\\
		0 \quad & \text{ else}.
		\end{cases}
	\]
	The two random variables, $x \and \alpha$ are independent (why?), so we can use the joint probability distribution:
	\[
		F(x,\alpha) = \begin{cases}
			\frac{4}{a\pi} \quad & 0 \le x \le \frac{a}{2},\;0\le \alpha \le \frac{\pi}{2}\\
			0 \quad & \text{ else}.
		\end{cases}
	\]
	The needle crosses a line, if
	\[
		x \le \frac{l}{x} \sin \alpha.
	\]
	Now only integrate over $F(x,\alpha)$ to get the probability that the needle will cross a line:
	\[
		P = \int_{\alpha = 0}^{\frac{\pi}{2}}\int_{x=0}^{(l/2)\sin \alpha} \frac{4}{a \pi} \d x \d \alpha = \frac{2l}{a \pi}.
	\] 
	The probability for this case is $P = \frac{2l}{a \pi}$.
\end{solution}