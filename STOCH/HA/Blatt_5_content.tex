% !TeX spellcheck = en_US
% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section{5th Homework STOCH}
%%%%%%%%%%%%%%%%%%%% Aufgabe 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof} %TODO restructure!
	The whole system fails if $\max(\min(T_A , T_B), \min(T_C, T_D))$, because we have pairs of components:
	\begin{align*}
		\text{outage of } [A,B] \text{ with } \min(T_A, T_B)\\
		\text{outage of } [C,D] \text{ with } \min(T_C, T_D)
	\end{align*}
	Because
	\begin{align*}
		T_k \sim \exp(\lambda) &\Longleftrightarrow \rho(x) = \lambda e^{-\lambda x} \quad \with k \in \set{A,B,C,D}\\
		&\Longleftrightarrow \text{ probability mass function}.
	\end{align*}
	\begin{align*} %TODO flip backward, because i use the claim :/
		\meas(T< t) &= \meas(A) = \int_A \rho(x) \d x = [-e^{-\lambda x}]_A \label{5_1:eq1}\tag{$\star$}\\
		&= [-e^{-2\lambda x}]_0^t = 1 - e^{-2\lambda t}
	\end{align*}
	Now we want to find out, how this looks for $T_{AB} \and T_{CD}$, so we need to square the integral \eqref{5_1:eq1}.
	\begin{align*}
		\meas(T<t) &= \brackets{\int_0^t 2 \lambda e^{-2 \lambda x} \d x}^2
		\\
		&= \meas(T_{AB} < t) \mal \meas(T_{CD}<t)\\
		&= \meas(T_{AB} < t, T_{CD} < t).
	\end{align*} 
	This means, that $T_{AB} \sim \Exp(\lambda + \lambda) = \Exp(2\lambda)$-distributed (analog for $T_{AB}$). So we can define $T_{AB} := T_A + T_B \sim \Exp(\lambda \lambda) = \Exp(2\lambda)$. Also important is $\meas(T_B < t+s) \mid T_A < s) = \meas(T_B <t)$.
	WLOG we can always assume that $A$ fails before $B$, same for the pair $C,D$ $T_{AB}$ describes the downtime point of the subsystem $AB$ and $T_{CD}$ the downtime point of the subsystem $CD$. Then we have always $T_{AB} = T_A \sim \Exp(\lambda)$.
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{solution}
	\begin{enumerate}
		\item We have for the $X,Y$-measures:
		\begin{align*}
			\meas_X ((0,\infty)) = \int_{0}^{\infty} e^{-x} \d x\\
			\meas_Y ((0,\infty)) = \int_{0}^{\infty} e^{-y} \d y
		\end{align*}
		because $X,Y \sim \Exp(1)$.
		We need to make a few convolutions to obtain the needed probability mass functions (pmfs):
		\begin{enumerate}
			\item $X+Y$ is first up
			\begin{align*}
				\rho_{X+Y}(x) &= \int_{0}^x \rho(x-y)\rho(y) \d y\\
				&= \lambda^2 \int_{0}^x e^{-\lambda(x-y)} e^{-\lambda y}\d y\\
				&= \lambda^2 e^{-\lambda x} \int_{0}^x \indi \d y\\
				&= \lambda^2 e^{-\lambda x} x \quad\text{ with } \lambda = 1 \text{ here.}
			\end{align*}
			\item $\frac{X}{X+Y}$ is the second convolution
			\begin{align*}
				\rho_{\frac{X}{X+Y}} (x) &= \int_0^x ...
			\end{align*}
			Should work with the transformation formula MINT prop 18.1 - Schilling. But it is far to late now to iterate through this.
		\end{enumerate}
		\item ... Would like to get a hint here, so I can try on my own ... :(
		\begin{align*}
			...
		\end{align*}
	\end{enumerate}
\end{solution}
%%%%%%%%%%%%%%%%%%%% Aufgabe 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof} Let $(\O, \F, \meas)$ be a probability space with a random variable $X$. To show that $X$ is independent of itself, if a constant $c$ exists, s.t. $\meas(X =c) = 1$ holds.
	\begin{enumerate}
		\item $\Longrightarrow$:
		For every measurable set $A \in \F$, applying the independence of events $\set{X \in A} \and \set{X \in A}$ yields
		\begin{align*}
		\meas(X \in A) = \meas(X \in A)\meas(X \in A) \implies \meas(X \in A) \in \set{0,1}.
		\end{align*}
		(The constants $c$ are 1 and 0.)\\
		Hence $\meas = \delta$ dirac measure and therefore $X$ is constant.
		\item $\Longleftarrow$: It holds
		\begin{align*}
		\meas(\underbrace{X \cap X}_{=X}) = \meas(X) = 1 = \meas(X=c)
		\end{align*}
		and this implies that $X$ is independent of itself and that is what we desired.
		\item (alternative idea:) Would be to use the expected value, but then we would require $X$ to be square integrable(not given). (Only as a side note.)
	\end{enumerate}
\end{proof}
%%%%%%%%%%%%%%%%%%%% Aufgabe 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	\begin{enumerate}
		\item To show: $\rho := \lambda + \mu - \lambda\mu \in (0,1)$
		\begin{align*}
			\lambda + \mu - \lambda\mu = \lambda(1-\mu) + \mu
		\end{align*}
		Now we see, that $(1-\mu) \in (0,1)$, $\lambda(1-\mu) \in (0,1)$ and also $\lambda\mu = \lambda(1-\mu) + \mu \in (0,1)$ and this implies $\rho \in (0,1)$. 
		\item Calculating the distribution of $Z := \min{X,Y}$.
			\begin{enumerate}
				\item The geometric distribution was given for $X,Y$ so we have the CDFs \label{prob:5_4:1}
				\begin{align*}
					F_X (k) = \meas(x \le k) = \sum_{k' = 1}^k \meas(x=k') = \sum_{k' = 1}^k \lambda (1- \lambda^{k' - 1}) \overset{(\ast)}{=} 1 - (1 -\lambda)^k\\
					F_Y (k) = \meas(y \le k) = \sum_{k' = 1}^k \meas(x=k') = \sum_{k' = 1}^k \mu (1- \mu^{k' - 1}) \overset{(\ast)}{=} 1 - (1 -\mu)^k
				\end{align*}
				$(\ast)$ follows simply from the finite geometric series.
				\item Because of \ref{prob:5_4:1} for $X$ or $Y$ follows \label{prob:5_4:2}
				\begin{align*}
				\meas(x > k) &= 1- \meas(X \le k) = 1-(1-(1-\lambda)^k) = (1-\lambda)^k\\
				\meas(y > k) &= 1 - \meas(y \le k) = (1-\mu)^k
				\end{align*}
				\item from \ref{prob:5_4:2} follows for $Z$: \label{prob:5_4:3}
				\begin{align*}
				\meas(Z > k) &= \meas(\min\set{X,Y} > k)\\
				&= \meas(X > k \and Y > k)\\
				\overset{(\star)}&{=} \meas(X > k)\meas(Y >k)\\
				&= (1-\lambda)^k (1-\mu)^k\\
				&= \set{(1-\lambda)(1-\mu)}^k
				\end{align*}
				\item And finally from \ref{prob:5_4:3} follows for $\meas(Z \le k)$:
				\begin{align*}
				\meas(Z \le K) &= 1 - \meas(Z \le k)\\
				&= 1 - \set{(1-\lambda)(1-\mu)}^k
				\end{align*}
				And this is the CDF (cummulative ditribution function) of $\min(x,y)$.
			\end{enumerate}
	\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{proof}
	The definition of expected value was:
	\[
	\E[X] = \int_{\O} X(\omega)\meas(\d \omega) = \int_{\R} \P(X \in \d x)
	\]
	\begin{enumerate}
		\item The density function of uniform distribution: 
		\[
			\rho(x) = \frac{1}{\lambda(\O)}
		\]
		With that given we can calculate the expected value for the uniform distribution:
		\begin{align*}
			\E[X] &= \int_{\R} \id_{\R} (X) \mal \rho(X) \d x\\
			&= \int_a^b x \mal \frac{1}{b-a} \d x\\
			&= \frac{1}{b-a} [\frac{1}{2} x^2]_a^b\\
			&= \frac{1}{2} \frac{(b+a)(b-a)}{(b-a)} = \frac{b+a}{2}
		\end{align*}
		\item From the lecture notes we get the definition of Gamma distribution (and the identity $\Gamma(r+1) = r\Gamma(r)$ we proved in MINT last term) and $X$ has the probability density function:
		% https://proofwiki.org/wiki/Expectation_of_Gamma_Distribution
		\[
			\rho_X (x) = \frac{\lambda^r x^{r -1}e^{-\lambda x}}{\Gamma(r)}
		\] with $\Gamma(r) = \int_{x=0}^{\infty} x^{r-1}e^{-x}\d x$ the Gamma function. And for the definition of the expected value of a continuous random variable we get
		\[
			\E[X] = \int_{x=0}^{\infty}x \rho_X (x) \d x
		\]
		Hence
		\begin{align*}
			\E [X] &= \frac{\lambda^r}{\Gamma(r)} \int_0^{\infty} x^r e^{-\lambda x} \d x\\
			&= \frac{\lambda^r}{\Gamma(r)} \int_0^{\infty} \frac{t}{\lambda}^{r} e^{-t} \frac{\d t}{\lambda}\quad \text{sub. } t = \lambda x\\
			&= \frac{\lambda^r}{\lambda^{r +1}\Gamma(r)}\int_0^{\infty} t^r e^{-t} \d t\\
			&= \frac{\Gamma(r+1)}{\lambda \Gamma(r)} \quad \text{ definition of gamma function}\\
			&= \frac{r \cancel{\Gamma(r)}}{\lambda \cancel{\Gamma(r)}}\\
			&= \frac{r}{\lambda}
		\end{align*}
	\end{enumerate}
\end{proof}

\subsection{*}
\begin{proof}
	Just a sketch right now, need to make it rigid, but it is so late already ...
	\begin{align*}
		\negBin(n,p) &= \binom{k+r+1}{1-p}^r p^k\\
		&= \frac{(k+r-1)!}{k!(r-1)!}\mal (1-p)^{\frac{1}{p}\mal rp}p^k\\
		&\approx \frac{(k+r-1)(k+r-2)\cdots(k+1)}{(r-1)!}\set{(1-p)^{\frac{1}{p}}}^{rp}p^k\\
		&\approx \frac{k^{r-1}}{\Gamma(r)}e^{-rp}p^k\\
		&= \Gam\brackets{r,\frac{1}{p}}
	\end{align*}
\end{proof}