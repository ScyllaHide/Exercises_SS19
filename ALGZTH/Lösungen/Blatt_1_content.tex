% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section*{Übung 1 - Körpergrad, algebraische Erweiterungen}
Sei $L \vert K$ Körpererweiterung.
\subsection*{Aufgabe V1}
Ist $\chara(K) = p >0$, so gilt $(a+b)^p = a^p + b^p$ für alle $a,b \in K$.
\begin{proof}
	Es seien $a,b \in K$. Es gilt:
	\begin{align*}
		(a+b)^p = \sum_{k=1}^p \frac{p!}{k! (p-k)!}a^k b^{p-k}
	\end{align*}
	Für $1 \le k \le p-1$ gilt $p \vert \frac{p!}{k! (p-k)!}a^k b^{p-k}$. Da $\chara(K) = p$ folgt $\frac{p!}{k! (p-k)!}a^k b^{p-k} = 0$ für $1 \le k \le p-1$. Deswegen ist $(a+b)^p = a^p + b^p$.
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Aufgabe V2}
Seien $\alpha \in L \ K$ und $\alpha^2 \in K$, so ist $[K[\alpha], K] = 2$. 
\begin{proof}
	Es gibt zwei Möglichkeiten:
	\begin{enumerate}
		\item \ul{Minimalpolynom:} Das Polynom $X^2+a^2$ ist irreduzibel, da $a \in L \ K$ ist.
		\item \ul{lineare Unabhängigkeit:} Die Elemente $1,a$ sind linear unabhängig., aber $1,a,a^2$ ist linear abhängig, also ist $1,a$ eine $K$-Basis von $K(a)$.
	\end{enumerate}
	Und beide Argumente geben uns die Behauptung.
\end{proof}
%%%%%%%%%%%%%%%%%%%% Aufgabe 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Aufgabe V3}\label{sec_V3}
Sei $\alpha \in L^{\times}$. Zeigen Sie: Für $a \in K^{\times}$ und $b \in K^{\times}$ ist $K[\alpha] = K[a\alpha + b]$ und $K(\alpha) = K(\alpha^{-1}) = K(a\alpha + b)$.

\begin{proof}
	\begin{itemize}
		\item $K[\alpha] = K[a\alpha + b]:$
		\begin{align*}
			&(\supseteq)\quad a\alpha + b = a(\alpha) + b\\
			&(\subseteq)\quad a\alpha = \frac{1}{a} (a\alpha + n) - \frac{b}{a}\\
		\end{align*}
		\item $K(\alpha) = K(\alpha^{-1}):$
		\begin{align*}
			&(\subseteq)\quad a\alpha^{-1} = \frac{1}{a}\\
			&(\supseteq)\quad a\alpha = (\alpha^{-1})^{-1}
		\end{align*}
		\item ``$K(\alpha) = K(\alpha +b)$'' folgt aus $K[\alpha] = K[\alpha + b]$ und der Definition von $K(...)$. 
	\end{itemize}
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Aufgabe Ü4}
Ist $[L:K] = p$ prim, so ist $L = K(\alpha)$ für jedes $\alpha \in L \setminus K$.

\begin{proof}
	Sei $\alpha \in L \vert K$. Es gilt
	\begin{align*}
		p = [L:K] = [L:K(\alpha)]\mal [K(\alpha):K]
	\end{align*}
	Da $p$ prim ist, gilt entweder $[L:K(\alpha)] = 1$ und $[K(\alpha):K] = p$ oder $[L:K(\alpha)]=p$ und $[K(\alpha):K] = 1$. Jedoch ist $\alpha \not\in K$, d.h. $K(\alpha) \neq K$ und somit $[K(\alpha):K] \neq 1$. Deswegen ist $[L:K(\alpha)] = 1$, d.h. $L = K(\alpha)$
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Aufgabe Ü5}
Bestimmen Sie das Minimalpolynom von $\sqrt[n]{2}$ über $\Q$, für $n \in \N$. Bestimmen Sie das Minimalpolynom von $1+\ii$ über $\R$.
\begin{proof}
	Es gibt zwei Möglichkeiten:
	\begin{enumerate}
		\item 
		\begin{enumerate}
			\item Sei $n \ge 1$. Es ist klar, dass $\sqrt[n]{2}$ eine Nullstelle von $X^2-2 \in \Q[X]$ ist. Diese Polynom ist normiert und irreduzibel über $\Q$ (EISENSTEIN, $p=2$). Deshalb ist $X^n -2$ das Minimalpolynom von $\sqrt[n]{2}$ über $\Q$.
			\item Schreibe $w = 1+\ii$. Es ist
			\begin{align*}
				w^2 + 1 -2w &=(w-1)^2 = \ii^2
				\intertext{d.h.}
				w^2 -2w +2 &= 0
			\end{align*}
			Deshalb ist $1+\ii$ eine Nullstelle von $f=X^2-2X +2 \in \R[X]$. Da $\deg(f) = 2$ und $f$ keine Nullstelle in $\R$ hat, ist $f$ irreduzibel über $\R$. Das Polynom $f$ ist auch normiert. Deshalb ist $f$ das Minimalpolynom von $1+\ii$ über $\R$.
		\end{enumerate}
		\item Mit \nameref{sec_V3} gilt $\R(1+\ii) = \R(\ii) = \C$. Deshalb ist $[\R(1+\ii)\colon \R] = 2$, d.h. das Minimalpolynom von $1+\ii$ über $\R$ hat Grad 2. Außerdem ist $f$ normiert und $1+\ii$ ist Nullstelle von $f$. Da $f$ Grad 2 hat, schließen wir, dass $f$ das Minimalpolynom von $1+\ii$ über $\R$ ist.
	\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Aufgabe Ü6}
Sei $\alpha \in L$ nicht algebraisch über $K$ und $n \ge 1$. Zeigen Sie, dass $[K(\alpha):K(\alpha^n)] = n$.

\begin{proof}
	Da $\alpha$ nicht algebraisch über $K$ ist, ist $\alpha^n$ nicht algebraisch über $K$, d.h. $K(\alpha^n) \cong K(T)$. Das Polynom $x^n - T$ ist irreduzibel über $K(T)$ nach EISENSTEIN. Deswegen ist $X^n - \alpha^n$ irreduzibel über $K(\alpha^n)$. Das Polynom $x^n - \alpha^n$ ist normiert und $\alpha^n$ ist eine Nullstelle dieses Polynoms. Deshalb ist $x^n - \alpha^n$ das Minimalpolynom von $\alpha$ über $K(\alpha^n)$. Insbesondere ist $[K(\alpha):K(\alpha)] = n$.
\end{proof}

%%%%%%%%%%%%%%%%%%%% Aufgabe 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Aufgabe P14}
Geben Sie ein Beispiel einer Körpererweiterung $L\vert K$, die endlich erzeugt aber nicht endlich ist.

\begin{lösung}
	Betrachte $\Q(x)\vert \Q$. Es ist $[\Q(x):\Q] = \infty$.\\
	$L = K(T)$ mit nicht algebraischem T.
\end{lösung}

%%%%%%%%%%%%%%%%%%%% Aufgabe 8 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Aufgabe P15} %TODO
\begin{enumerate}
	\item $\Q(\sqrt{2}) = \Q(\sqrt{3})$? \label{ex:P15:1}
	\item $\Q(\sqrt{2}) \cong \Q(\sqrt{3})$? \label{ex:P15:2}
\end{enumerate}

\begin{lösung}
	zu \ref{ex:P15:1}: Es ist $\sqrt{2} \in \Q$. Existiert also ein $a+b\sqrt{3} \in \Q(\sqrt{3})$. Sodass $\sqrt{2} = a + b\sqrt{3}$ gibt. Da $\sqrt{2} \not\in \Q$ muss $b \neq 0$.
	\begin{align*}
		[\Q(\sqrt{2}):\Q(\sqrt{3})] = 1 \Rightarrow \sqrt{2} - (a+b\sqrt{3}) = 0\text{?}
	\end{align*}
\end{lösung}

%%%%%%%%%%%%%%%%%%%% Aufgabe 8 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Aufgabe P16} %TODO

%\end{document}

%\begin{align*}
%	L_0^2:=\big\lbrace X\in L_2:\E[X]=0\big\rbrace
%\end{align*}
%der Raum der zentrierten quadrat-integrierbaren Zufallsvariablen. 
%Für diesen gilt:
%\begin{enumerate}[label=\alph*)]
%	\item $L_0^2$ ist ein Hilbertraum mit Skalarprodukt $\Cov(\cdot,\cdot)$.
%	\item Seien $X_1,...,X_n\in L_0^2$. Dann gilt:
%	\begin{align*}
%		\Var\left[\sum\limits_{j=1}^n X_j\right]
%		=\sum\limits_{j=1}^n\left(\Var[X_j]+2\cdot\sum\limits_{k>j}\Cov[X_j,X_k]\right)
%	\end{align*}
%	\item Seien $X_1,...,X_n\in L_0^2$. Dann gilt:
%	\begin{align*}
%		\sqrt{\Var\left[\sum\limits_{j=1}^n X_j\right]}\leq\sum\limits_{j=1}^n \sqrt{\Var[X_j]}
%	\end{align*}
%\end{enumerate}
%
%\begin{proof}
%	\underline{Zeige a):}\\
%	Nutze Untervektorraumkriterium:
%	\begin{itemize}
%		\item $0\in L_0^2$ ist klar.
%		\item Sei $X\in L_0^2$ und $\lambda\in\R$. Dann ist $X\in L_0^2$ wegen
%		\begin{align*}
%			\E[\lambda\cdot X]\stackeq{\text{Lin}}\lambda\cdot\E[X]\stackeq{X\in L_2^0}0
%		\end{align*}
%		\item Seien $X,Y\in L_0^2$. Dann ist $X+Y\in L_0^2$ wegen
%		\begin{align*}
%			\E[X+Y]\stackeq{\text{Lin}}\E[X]+\E[Y]\stackeq{X,Y\in L_0^2}0+0=0
%		\end{align*}
%	\end{itemize}
%	Somit ist $L_0^2$ ein Untervektorraum von $L_2$. Dass
%	\begin{align*}
%		\langle X,Y\rangle:=\Cov(X,Y):=
%		\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
%		=\E[X\cdot Y]
%		\qquad\forall X,Y\in L_0^2
%	\end{align*}
%	ein Skalarprodukt auf $L_0^2$ ist, ist klar, das es mit dem $L^2$-Skalarprodukt des Oberraumes $L^2$ übereinstimmt.
%
%	%Gleich vorweg: Die Kovarianz erfüllt sogar auf $L^2$ alle Eigenschaften bis auf positive Definitheit. Diese gilt nur auf $L_0^2$. Deswegen werden im Folgenden die Eigenschaften der Kovarianz allgemeiner gezeigt.
%	%\begin{itemize}
%	%\item Die Kovarianz ist Bilinear, denn für $\lambda\in\R$ und $X,Y,Z\in L_0^2$ gilt
%	%\begin{align*}
%	%\Cov(\lambda\cdot X,Y)
%	%&=\E\Big[\big(\lambda\cdot X-\E[ \lambda\cdot X]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\E\Big[\big(\lambda\cdot\big(X-\E[X]\big)\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\lambda\cdot\Cov(X,Y)\\
%	%\Cov(X,\lambda\cdot Y)
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(\lambda\cdot Y-\E[\lambda\cdot Y]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(\lambda\cdot\big(Y-\E[Y]\big)\big)\Big]\\
%	%&=\lambda\cdot\Cov(X,Y)\\
%	%\Cov(X+Z,Y)
%	%&=\E\Big[\big(X+Z-\E[X+Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)+\big(Z-\E[Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
%	%+\E\Big[\big(Z-\E[Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
%	%&=\Cov(X,Y)+\Cov(Z,Y)\\
%	%\Cov(X,Y+Z)
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y+Z-\E[Y+Z]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)+\big(X-\E[X]\big)\cdot\big(Z-\E[Z]\big)\Big]\\
%	%&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
%	%+\E\Big[\big(X-\E[X]\big)\cdot\big(Z-\E[Z]\big)\Big]\\
%	%&=\Cov(X,Y)+\Cov(X,Z)
%	%\end{align*}
%	%\item Die Kovarianz ist symmetrisch, was man schon an der Definition erkennt: $\Cov(X,Y)=\Cov(Y,X)$
%	%\item Die Kovarianz ist positiv definit:
%	%\begin{align*}
%	%\Cov(X,X)&=\E\Big[\underbrace{\big(X-\E[X]\big)^2}_{\geq0}\Big]\geq0\text{ und}\\
%	%\Cov(X,X)&=0\stackrel{\E[X]=0}{\gdw} X=0\text{ fast sicher}
%	%\end{align*}
%	%\end{itemize}
%	
%	Bleibt noch zu zeigen, dass $L_0^2$ bzgl. der die die Kovarianz induzierten Norm
%	\begin{align*}
%		\Vert X\Vert:=\sqrt{\Cov(X,X)}
%		=\sqrt{\E\Big[\big(X-\E[X]\big)^2\Big]}
%		=\sqrt{\E\Big[X^2\Big]}
%		\qquad\forall X,Y\in L_0^2
%	\end{align*}
%	vollständig ist. 
%	Diese Norm stimmt auf $L_0^2$ mit der bekannten $L^2$-Norm überein. 
%	Und da bekannt ist, dass $\big(L^2,\Vert\cdot\Vert_{L^2}\big)$ ein Banachraum ist, genügt es noch zu zeigen, dass $L_0^2$ abgeschlossen ist.
%	Sei also $(X_n)_{n\in\N}\subseteq L_0^2$ eine Folge mit
%	$X_n\stackrel{n\to\infty}{\longrightarrow}X\in L^2$.
%	Dann gilt schon $X\in L_0^2$, denn:
%
%	%Angenommen $\E[X]\neq0$. Dann gibt es $c>0$ so, dass
%	%\begin{align*}
%	%0<c\leq \big(\E[X]\big)^2&=\Big(\E[X]\cdot\big(\E[X-X_n+X_n]\big)\Big)\\
%	%&=\big(\E[X]\cdot\E[X-X_n]+\E[X]\cdot\underbrace{\E[X_n]}_{=0}\big)\\
%	%&=\E[X]\cdot\E[X-X_n]\\
%	%&\leq\E\big[|X|\big]\cdot\E\big[|X-X_n|\big]
%	%\stackrel{n\to\infty}{\longrightarrow}0
%	%\end{align*}
%	%Dies ist ein Widerspruch. Folglich $\E[X]=0$ und somit $X\in L_0^2(\Omega)$.\\
%
%	\begin{align*}
%		E[X]
%		&=\E[X-X_n+X_n]\\ 
%		&=\E[X-X_n]+\underbrace{\E[X_n]}_{=0}\\
%		\implies
%		\big|\E[X]\big|&\leq \E\big[|X-X_n|\big]
%		\overset{\text{Jensen}}{\leq}
%		\sqrt{\E\big[(X_n-X)^2\big]}
%		\overset{n\to\infty}{\longrightarrow} 0\\
%		\implies
%		\E[X]&=0
%	\end{align*}
%
%	\underline{Zeige b):} 
%	Für $X,Y\in L_0^2$ gilt $\Var[X]=\E\big[X^2\big]$ und $\Cov[X,Y]=\E[X\cdot Y]$. 
%	Damit folgt
%	\begin{align*}
%		\Var\left[\sum\limits_{j=1}^n X_j\right]
%		&=\E\left[\left(\sum\limits_{j=1}^n X_j\right)^2\right]\\
%		&=\E\left[\sum\limits_{j=1}^n\sum\limits_{k=1}^n X_j\cdot X_k\right]\\
%		&=\sum\limits_{j=1}^n\sum\limits_{k=1}^n \E[X_j\cdot X_k]\\
%		&=\sum\limits_{j=1}^n\left(
%		\sum\limits_{\begin{subarray}{c}k=1\\ k=j\end{subarray}}^n \E[X_j\cdot X_k]
%		+\sum\limits_{\begin{subarray}{c}k=1\\ k\neq j\end{subarray}}^n \E[X_j\cdot X_k]\right)\\
%		&=\sum\limits_{j=1}^n\left(\underbrace{\E\big[X_j^2\big]}_{=\Var[X]}+2\cdot\sum\limits_{k>j}\underbrace{\E[X_j\cdot X_k]}_{=\Cov(X_j,Y_k)}\right)
%	\end{align*}
%
%	Abstrakt als Hilbertraum aufgefasst:
%	\begin{align*}
%		\left\langle\sum\limits_{k},\sum\limits_{k} X_k\right\rangle
%		=\sum\limits_k\left\langle X_k,X_k\right\rangle+2\cdot\sum\limits_{i<j}\left\langle X_i,X_j\right\rangle
%	\end{align*}
%
%	\underline{Zeige c):}
%	\begin{align*}
%		\sqrt{\Var\left[\sum\limits_{j=1}^n X_j\right]}
%		=\sqrt{\E\left[\left(\sum\limits_{j=1}^n\right)^2\right]}
%		=\left\Vert\sum\limits_{j=1}^n X_i\right\Vert
%		\overset{\Delta\text{-Ungl}}{\leq}
%		\sum\limits_{j=1}^n\Vert X_j\Vert
%		=\sum\limits_{j=1}^n\sqrt{\Var[X_j]}
%	\end{align*}
%\end{proof}
%
%\subsection{Aufgabe 1.2}
%Seien $X,Y\in L_2(\A)$ und $\F\subseteq\A$ Unter-$\sigma$-Algebra von $\A$. 
%\textbf{Bedingte Varianz} und \textbf{bedingte Kovarianz} von $X$ bzw. $X,Y$ unter $\F$ sind definiert als
%\begin{align*}
%	\Var[X~|~\F]&:=\E\Big[\big(X-\E[X~|~\F]\big)^2~\Big|~\F\Big]\\
%	\Cov[X,Y~|~\F]&:=\E\Big[\big(X-\E[X~|~\F]\big)\cdot\big(Y-\E[Y~|~\F]\big)~\Big|~\F\Big]
%\end{align*}
%Dann gelten die Sätze der \textbf{totalen Varianz} bzw. \textbf{totalen Kovarianz}:
%\begin{align*}
%	\Var[X]&=\E\big[\Var[X~|~\F]\big]+\Var\big[\E[X~|~\F]\big]\\
%	\Cov[X,Y]&=\E\big[\Cov[X,Y~|~\F]\big]+\Cov\big[\E[X~|~F],\E[Y~|~\F]\big]
%\end{align*}
%
%\begin{proof}
%	\underline{Zur ersten Gleichung:}\\
%	Aus der Nebenrechnung
%	\begin{align}\label{eqTotaleVarianz}
%		\Var[X~|~\F]
%		\overset{\text{Def}}&=
%		\E\Big[\big(X-\E[X~|~\F]\big)^2~\Big|~\F\Big]\\\nonumber
%		\overset{\text{}}&=
%		\E\Big[X^2-2\cdot X\cdot \E[X~|~\F]+\big(\E[X~|~\F]\big)^2~\Big|~\F\Big]\\\nonumber
%		\overset{\text{Lin}}&=
%		\E\big[X^2~\big|~\F\big]-2\cdot\E\big[X\cdot\E[X~|~\F]\big]+\E\Big[\big(\E[X~|~\F]\big)^2~\Big|~\Big]\\\nonumber
%		\overset{\text{Pull-out}}&=
%		\E\big[X^2~\big|~\F\big]-2\cdot\E[X~|~\F]\cdot\E[X~|~\F]+
%		\E[X~|~\F]\cdot\underbrace{\E\big[\E[X~|~\F]~\big|~\F\big]}_{\stackeq{\text{Tower}}\E[X~|~\F]}\\\nonumber
%		&=
%		\E\big[X^2~\big|~\F\big]-\big(\E[X~|~\F]\big)^2 
%	\end{align}
%	folgt
%	\begin{align*}
%		&\E\big[\Var[X~|~\F]\big]+\Var\big[\E[X~|~\F]\big]\\
%		&=\E\big[\Var[X~|~\F]\big]+\E\Big[\big(\E[X~|~\F]\big)^2\Big]-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		\overset{\eqref{eqTotaleVarianz}}&=
%		\E\Big[\E\big[X^2~\big|~\F\big]-\big(\E[X~|~\F]\big)^2 \Big]+
%		\E\Big[\big(\E[X~|~\F]\big)^2\Big]-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		\overset{\text{Lin}}&=
%		\E\Big[\E\big[X^2~\big|~\F\big]\Big]\underbrace{-\E\Big[\big(\E[X~|~\F]\big)^2 \Big]+
%		\E\Big[\big(\E[X~|~\F]\big)^2\Big]}_{=0}-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		&=\E\Big[\E\big[X^2~\big|~\F\big]\Big]-\Big(\E\big[\E[X~|~\F]\big]\Big)^2\\
%		\overset{\text{Tower}}&=
%		\E[X^2]-\big(\E[X]\big)^2\\
%		&=\Var[X]
%	\end{align*}
%
%	\underline{Zur zweiten Gleichung:} 
%
%	Zunächst gilt die \textit{Polarisation:} 
%	\begin{align*}
%		\Cov(X,Y)=\frac{1}{2}\cdot\big(\underbrace{\Var[X+Y]}_{=\langle X+Y,X+Y\rangle}-\underbrace{\Var[X]}_{=\langle X,X\rangle}-\underbrace{\Var[Y]}_{=\langle Y,Y\rangle}\big)
%	\end{align*}
%	Setze der Kürze halber $X_F:=\E[X~|~\F], Y_F:=\E[Y~|~\F]$. 
%	Dann gilt:
%	\begin{align*}
%		&\E[\Cov(X,y~|~\F)]\\
%		&=\E[\E[(X-X_F)\cdot(Y-Y_F)~|~\F]]\\
%		\overset{\text{Tower}}&=
%		\E[(X-X_F)\cdot(Y-Y_F)]\\
%		&=\E[X\cdot Y]-\E[\E[X~|~F]\cdot Y]-\E[X\cdot\E[Y~|~\F]]+\E[\E[X~|~\F]\cdot\E[Y~|~\F]]\\
%		\overset{\text{Sym}}&=
%		\E[X\cdot Y]-\E[X_F\cdot Y_F]\\
%		&=\Cov(X,Y)+\E[X]\cdot\E[Y]-\E[X_F\cdot Y_F]\\
%		\overset{\text{Tower}}&=
%		\Cov(X,Y)+\E[X_F]\cdot\E[Y_F]-\E[X_F\cdot Y_F]\\
%		&=\Cov(X,Y)-\Cov(\E[X~|~\F],\E[Y~|~\F]]
%	\end{align*}
%	Umstellen impliziert die Behauptung.
%\end{proof}
%
%\subsection{Aufgabe 1.3}
%Sei 
%$M=\begin{pmatrix}
%	A & B\\ C & D
%\end{pmatrix}$ eine quadratische, in Blöcke unterteilte Matrix von vollem Rang, wobei $A$ und $D$ ebenfalls quadratisch und von vollem Rang seien. 
%Die Ausdrücke
%\begin{align*}
%	(M/A):=\left(D-C\cdot A^{-1}\cdot B\right),\qquad(M/D):=\left(A-B\cdot D^{-1}\cdot C\right)
%\end{align*}
%heißen \textbf{Schurkomplement} von $A$ in $M$ bzw. 
%$D$ in $M$. Dann gilt:
%\begin{enumerate}[label=\alph*)]
%	\item $\begin{aligned}
%		M^{-1}=\begin{pmatrix}
%			(M/D)^{-1} & -A^{-1}\cdot B\cdot(M/A)^{-1}\\
%			-D^{-1}\cdot C\cdot(M/D)^{-1} & (M/A)^{-1}
%		\end{pmatrix}
%	\end{aligned}$
%	\item Sei $M$ nun symmetrisch, d.h. $A$ und $D$ sind symmetrisch und $C=B^T$. Dann gilt:
%	\begin{align*}
%		(x^T,y^T)\cdot M^{-1}\cdot\begin{pmatrix}
%			x\\y
%		\end{pmatrix}-y^T\cdot D^{-1}\cdot y=\tilde{x}^T\cdot(M/D)^{-1}\cdot\tilde{x}
%	\end{align*}
%	mit $\tilde{x}=\left(x-B\cdot D^{-1}\cdot y\right)$ für alle $x,y$ mit passender Dimension.
%	\item Es sei $(X,Y)$ multivariat normalverteilt mit Erwartungswert 0 und positiv definiter Kovarianzmatrix 
%	$\Sigma=\begin{pmatrix}
%		\Sigma_X & \Sigma_{XY}\\ 
%		\Sigma_{XY}^T & \Sigma_Y
%	\end{pmatrix}$. 
%	Dann ist $X$ bedingt auf $Y$ normalverteilt mit $\E[X~|~Y]=\Sigma_{XY}\cdot\Sigma_{Y}^{-1}$ und Kovarianzmatrix $(\Sigma/\Sigma_Y)$\\
%	Hinweis: Es gilt $\det(\Sigma)=\det(\Sigma_Y)\cdot\det(\Sigma/\Sigma_Y)$.
%\end{enumerate}
%
%\begin{proof}
%	\underline{Zeige a):}\\
%	Beachte: Matrixmultiplikation gilt Blockweise.
%	\begin{align*}
%		&M\cdot M^{-1}\\
%		&=\begin{pmatrix}
%			A & B\\ C & D
%		\end{pmatrix}\cdot\begin{pmatrix}
%			(A-B\cdot D^{-1}\cdot C)^{-1} & -A^{-1}\cdot B\cdot(D-A\cdot C^{-1}\cdot B)^{-1}\\
%			-D^{-1}\cdot C\cdot(A-B\cdot D^{-1}\cdot C)^{-1} & (D-C\cdot A^{-1}\cdot B)^{-1}
%		\end{pmatrix}\\
%		&=\begin{pmatrix}
%			(\ast) & 0\\
%			0 & (\ast\ast)
%		\end{pmatrix}\\
%		(\ast)
%		&=A\cdot(A-B\cdot D^{-1}\cdot C)^{-1}-B\cdot D^{-1}\cdot C(A-B\cdot D{-1}\cdot C)^{-1}\\
%		&=(A-B\cdot D^{-1}\cdot C)\cdot(A-B\cdot D^{-1}\cdot C)^{-1}\\
%		&=I\\
%		(\ast\ast)
%		&=(-C\cdot A^{-1}\cdot B+D)\cdot(D-C\cdot A^{-1}\cdot B)^{-1}\\
%		&=I
%	\end{align*}
%
%	\underline{Zeige b):}
%	\begin{align}\label{3b1}\tag{$\ast$} 
%		&-D^{-1}\cdot B^T\cdot(M/D)^{-1}
%		=-(M/A)^{-1}\cdot B^T\cdot A^{-1}\\\nonumber
%		&\implies
%		(M/A)\cdot D^{-1}\cdot B^T=B^T\cdot A^{-1}\cdot(M/D)\\\nonumber
%		&\implies
%		(D-B^T\cdot A^{-1}\cdot B)\cdot D^{-1}\cdot B^T=B^T\cdot A^{-1}\cdot(A-B\cdot D^{-1}\cdot B^T)\\\nonumber
%		&\implies
%		B^T-B^T\cdot A^{-1}\cdot B\cdot D^{-1}\cdot B^T
%		=B^T-B^T\cdot A^{-1}\cdot B\cdot D^{-1}\cdot B^T
%	\end{align}
%
%	\begin{align*}
%		M^{-1}
%		&=\begin{pmatrix}
%			(M/D)^{-1} & -A^{-1}\cdot B\cdot(M/A)^{-1}\\
%			-D^{-1}\cdot B^T\cdot(M/D)^{-1} & (M/A)^{-1}
%		\end{pmatrix}\\
%		\overset{\eqref{3b1}}&=
%		\begin{pmatrix}
%			(M/D)^{-1} & -A^{-1}\cdot B\cdot(M/A)^{-1}\\
%			-(M/A)^{-1}\cdot B^T\cdot A^{-1} & (M/A)^{-1}
%		\end{pmatrix}
%	\end{align*}
%	Zu zeigen:
%	\begin{align*}
%		\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}\cdot M^{-1}\cdot\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}-y^T\cdot D^{-1}\cdot y=\tilde{x}^T\cdot (M/D)^{-1}\cdot\tilde{x}
%		\mit\tilde{x}=x-B\cdot D^{-1}\cdot y
%	\end{align*}
%	Linke Seite:
%	\begin{align*}
%		x^T\cdot(M/D)^{-1}\cdot x-2 y^T\cdot\underbrace{(M/A)^{-1}\cdot B^T\cdot x}_{\text{wegen \eqref{3b1}}}\cdot x+y^T\cdot(M/A)^{-1}\cdot y-y^T\cdot D^{-1}\cdot y
%	\end{align*}
%	Rechte Seite:
%	\begin{align*}
%		x^T\cdot(M/D)^{-1}\cdot x-2 y^T\cdot D^{-1}\cdot B^T\cdot(M/D)^{-1}\cdot x+\\
%		+y^T\cdot D^{-1}\cdot B^T\cdot(M/D)^{-1}\cdot B\cdot D^{-1}\cdot y
%	\end{align*}
%	Noch zu zeigen
%	\begin{align*}
%		(M/A)^{-1}-D^{-1}
%		&=D^{-1}\cdot B^T\cdot(M/D)^{-1}\cdot B\cdot D^{-1}\\
%		&\implies %von links \cdot(M/A) und von rechts \cdot D
%		D-(M/A)=(M/A)\cdot\underbrace{ D^{-1}\cdot B^T\cdot(M/D)^{-1}}_{\stackrel{\eqref{3b1}}(M/A)^{-1}\cdot B^T\cdot A^{-1}}\cdot B
%		\\
%		&\implies
%		B^T\cdot A^{-1}\cdot B=B^T\cdot A^{-1}\cdot B
%	\end{align*}
%	
%	\underline{Zeige c):}
%	\begin{align*}
%		f_{X|Y}(x,y)
%		&=\frac{f_{XY}(x,y)}{f_Y(y)}\\
%		&=\frac{(2\cdot\pi)^{-\frac{m+n}{2}}\cdot\det(\Sigma_Y)}{\det(\Sigma)\cdot(2\cdot\pi)^{-\frac{n}{2}}}
%		\cdot\exp\left(-\frac{1}{2}\cdot\left(\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}^T\cdot\Sigma^{-1}\cdot\begin{pmatrix}
%			x\\ 
%			y
%		\end{pmatrix}-y^T\cdot\Sigma_Y\cdot y\right)\right)\\
%		\overset{\text{(b)}}&=
%		\frac{(2\cdot\pi)^{-\frac{m}{2}}}{\det(\Sigma/\Sigma_Y}\cdot\exp\left(-\frac{1}{2}\cdot\tilde{x}^T\cdot(\Sigma/\Sigma_Y)^{-1}\cdot\tilde{x}\right)
%		\mit\tilde{x}=x-\Sigma_{XY}\cdot\Sigma_{Y}^{-1}\cdot y
%	\end{align*}
%	ist Dichte von 
%	\begin{align*}
%		\mathcal{N}\Big(\Sigma_{XY}\cdot\Sigma_Y^{-1}\cdot y,(\Sigma/\Sigma_Y)\Big)
%	\end{align*}
%\end{proof}
%
%\subsection{Aufgabe 1.4}
%In einer bestimmten Population sei das Alter $X$ bei erstmaliger Berufsunfähigkeit exponentialverteilt mit Parameter $\lambda>0$. 
%Für eine Versicherungsgesellschaft die gegen Berufsunfähigkeit versichert ist das mittlere Alter bei Eintritt der Berufsunfähigkeit von Bedeutung, 
%unter der Bedingung dass die Berufsunfähigkeit zwischen den Altersgrenzen $0\leq a\leq b$ eintritt.\\
%Bestimme diesen bedingten Erwartungswert $\E[X~|~a\leq X\leq b]$.
%
%\begin{proof}
%	Die Zufallsgröße $X\colon\Omega\to\R$ hat Dichte $f_X(x)=\lambda\cdot\exp(-\lambda\cdot x)$
%	\begin{align*}
%		\E[X~|~a\leq X\leq b]
%		&=\frac{\E\left[X\cdot\indi_{\lbrace a\leq X\leq b\rbrace}\right]}{\P[a\leq X\leq b]}\\
%		&=\frac{\int\limits_a^b \lambda\cdot x\cdot\exp(-\lambda\cdot x)\d x}{\int\limits_a^b\lambda\cdot\exp(-\lambda\cdot x)\d x}\\
%		&=\frac{\int\limits_a^b x\cdot\exp(-\lambda\cdot x)\d x}{\int\limits_a^b\exp(-\lambda\cdot x)\d x}\\
%		&=\frac{\left[\left(-\frac{x}{\lambda}-\frac{1}{\lambda^2}\right)\cdot\exp(-\lambda\cdot x)\right]_{x=a}^b}
%		{\left[-\frac{\exp(-\lambda\cdot x)}{\lambda}\right]_{x=a}^b}\\
%		%&=\frac{
%		%\frac{b\cdot\exp(-\lambda\cdot b)}{\lambda}
%		%+\frac{\exp(-\lambda\cdot b)}{\lambda^2}
%		%-\frac{a\cdot\exp(-\lambda\cdot a)}{\lambda}
%		%-\frac{\exp(-\lambda)\cdot a)}{\lambda^2}
%		%}{
%		%\frac{\exp(-\lambda\cdot b)}{\lambda}
%		%-\frac{\exp(-\lambda\cdot a)}{\lambda}}\\
%		%&=\frac{\left(b+\frac{1}{\lambda}\right)\cdot\exp(-\lambda\cdot b)-\left(a+\frac{1}{\lambda}\right)\cdot\exp(-\lambda\cdot a)}{\exp(-\lambda\cdot b)-\exp(-\lambda\cdot a)}
%		&=\frac{\frac{(a\cdot\lambda+1)\cdot\exp(-\lambda\cdot a)}{\lambda^2}-\frac{(b\cdot\lambda+1)\cdot\exp(-\lambda\cdot b)}{\lambda^2}}{\frac{\exp(-\lambda\cdot a)}{\lambda}-\frac{\exp(-\lambda\cdot b)}{\lambda}}\\
%		&=\frac{(b\cdot\lambda+1)\cdot\exp(a\cdot\lambda)-(a\cdot\lambda+1)\cdot\exp(b\cdot\lambda)}{\big(\exp(a\cdot\lambda)-\exp(b\cdot\lambda)\big)\cdot\lambda}\\
%		\overset{\text{Prof}}&=
%		\frac{1}{\lambda}+\frac{a\cdot\exp(-\lambda\cdot a)-b\cdot\exp(-\lambda\cdot b)}{\exp(-\lambda\cdot a)-\exp(-\lambda\cdot b)}
%	\end{align*}
%\end{proof}
%
%\subsection{Aufgabe 1.5}
%Welche der folgenden in der Vorlesung definierten mathematischen Objekte sind: 
%reelle Zahlen, Zufallsvariablen, messbare Funktionen von $\R\to\R$?
%\begin{align*}
%	\E[X~|~\F],\qquad\P[A~|~B],\qquad \E[X~|~Y=y],\qquad\P[A~|~\F],\qquad\E[X~|~Y]
%\end{align*}
%Wie üblich bezeichnet $\F$ eine $\sigma$-Algebra, $X,Y$ Zufallsvariablen, $y$ eine reelle Zahl und $A,B$ Ereignisse.
%
%\begin{lösung}
%	\begin{align*}
%		&\E[X~|~\F]\in L_2(\Omega,\F,\P)\text{, ist also eine Zufallsvariable}\\
%		&\P[A~|~B]\in[0,1]\subseteq\R\\
%		&\E[X~|~Y=y]:=\int\limits_{\R^m}x\cdot f_{X|Y}(x,y)\d x\in\R\\
%		&\P[A~|~\F]:=\E[\indi_A~|~\F]\in L_2(\Omega,\F,P)\text{ bzw. }\in(0,1)\text{ für festes }\omega\in\Omega\\
%		&\E[X~|~Y]:=\E[X~|~\sigma(Y)]\in L_2(\Omega,\F,\P)\text{, ist also ein Spezialfall der ersten Zeile}
%	\end{align*}
%\end{lösung}
